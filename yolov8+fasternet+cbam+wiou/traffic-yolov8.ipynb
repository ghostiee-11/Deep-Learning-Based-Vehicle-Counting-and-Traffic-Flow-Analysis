{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12126135,"sourceType":"datasetVersion","datasetId":7635546},{"sourceId":12196483,"sourceType":"datasetVersion","datasetId":7682690}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.011897Z","iopub.status.idle":"2025-06-17T16:44:59.012108Z","shell.execute_reply.started":"2025-06-17T16:44:59.012012Z","shell.execute_reply":"2025-06-17T16:44:59.012021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CODE CELL 1: SETUP & CONFIGURATION\n\n# --- 1. INSTALL ALL DEPENDENCIES ---\nprint(\"Installing all required libraries...\")\n!pip install ultralytics opencv-python shapely deep-sort-realtime -q\nprint(\"✅ Installation complete.\")\n\n# --- 2. DATA UNIFICATION (WITH THE CORRECTED FUNCTION) ---\nimport os\nimport glob\nimport shutil\nimport yaml\nprint(\"\\n--- Starting Multi-Class Data Unification ---\")\n\n# This is the corrected, readable, and functional version\ndef process_and_copy_files(source_dir, dest_img_dir, dest_lbl_dir, prefix, source_map):\n    # First, check if the main source directory exists.\n    if not os.path.exists(source_dir):\n        print(f\"⚠️ Warning: Source directory not found: {source_dir}. Skipping.\")\n        return\n\n    # Define the paths for images and labels subdirectories.\n    # This now runs correctly after the check above passes.\n    img_path = os.path.join(source_dir, 'images')\n    lbl_path = os.path.join(source_dir, 'labels')\n\n    # Second, check if the essential 'images' subdirectory exists.\n    if not os.path.exists(img_path):\n        print(f\"⚠️ Warning: Images subdirectory not found in {source_dir}. Skipping.\")\n        return\n\n    # Proceed with file processing\n    image_files = glob.glob(os.path.join(img_path, '*.jpg')) + glob.glob(os.path.join(img_path, '*.png'))\n    print(f\"Processing {len(image_files)} files from {source_dir}...\")\n    for img_file in image_files:\n        name, _ = os.path.splitext(os.path.basename(img_file))\n        lbl_file = os.path.join(lbl_path, name + '.txt')\n        dest_img_file = os.path.join(dest_img_dir, prefix + name + os.path.splitext(img_file)[1])\n        dest_lbl_file = os.path.join(dest_lbl_dir, prefix + name + '.txt')\n        shutil.copy(img_file, dest_img_file)\n        if os.path.exists(lbl_file):\n            with open(lbl_file, 'r') as f_in, open(dest_lbl_file, 'w') as f_out:\n                for line in f_in:\n                    parts = line.strip().split()\n                    if len(parts) != 5: continue\n                    original_id = int(parts[0]); class_name = source_map.get(original_id)\n                    if class_name in FINAL_CLASS_IDS: new_id = FINAL_CLASS_IDS[class_name]; parts[0] = str(new_id); f_out.write(' '.join(parts) + '\\n')\n\nFINAL_CLASS_NAMES=['car','truck','bus','motorcycle']\nCLASS_MAP={'traffic_wala':{0:'car',1:'motorcycle',2:'truck',3:'bus'},'bdd':{1:'car',2:'truck',3:'bus',4:'motorcycle'}}\nFINAL_CLASS_IDS={name:i for i,name in enumerate(FINAL_CLASS_NAMES)}\nsource_1_path='/kaggle/input/traffic-dataset/traffic_wala_dataset/'\noutput_path='/kaggle/working/merged_data/'\ntrain_img_out,train_lbl_out=os.path.join(output_path,'train/images/'),os.path.join(output_path,'train/labels/')\nvalid_img_out,valid_lbl_out=os.path.join(output_path,'valid/images/'),os.path.join(output_path,'valid/labels/')\nos.makedirs(train_img_out,exist_ok=True);os.makedirs(train_lbl_out,exist_ok=True);os.makedirs(valid_img_out,exist_ok=True);os.makedirs(valid_lbl_out,exist_ok=True)\nprocess_and_copy_files(os.path.join(source_1_path,'train'), train_img_out, train_lbl_out, 'traffic_', CLASS_MAP['traffic_wala'])\nprocess_and_copy_files(os.path.join(source_1_path,'valid'), valid_img_out, valid_lbl_out, 'traffic_', CLASS_MAP['traffic_wala'])\nprint(\"--- Data Unification Complete ---\")\n\n# --- 3. CREATE CONFIG FILES ---\ndata_yaml_content={'train':'/kaggle/working/merged_data/train/images','val':'/kaggle/working/merged_data/valid/images','nc':len(FINAL_CLASS_NAMES),'names':FINAL_CLASS_NAMES}\nDATA_YAML_PATH = '/kaggle/working/vehicle_data.yaml'\nwith open(DATA_YAML_PATH, 'w') as f: yaml.dump(data_yaml_content, f, default_flow_style=False)\nprint(f\"✅ Created data config at: {DATA_YAML_PATH}\")\nCUSTOM_MODEL_YAML_CONTENT = f\"\"\"\nnc: {len(FINAL_CLASS_NAMES)}\ndepth_multiple: 1.00\nwidth_multiple: 1.00\nbackbone:\n  - [-1, 1, Conv, [64, 3, 2]]\n  - [-1, 1, Conv, [128, 3, 2]]\n  - [-1, 3, C2f, [128, True]]\n  - [-1, 1, Conv, [256, 3, 2]]\n  - [-1, 6, C2f, [256, True]]\n  - [-1, 1, Conv, [512, 3, 2]]\n  - [-1, 6, C2f, [512, True]]\n  - [-1, 1, Conv, [1024, 3, 2]]\n  - [-1, 3, C2f, [1024, True]]\n  - [-1, 1, SPPF, [1024, 5]]\nhead:\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 6], 1, Concat, [1]]\n  - [-1, 3, C2f, [512]]\n  - [-1, 1, nn.Dropout, [0.1]]\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 4], 1, Concat, [1]]\n  - [-1, 3, C2f, [256]]\n  - [-1, 1, nn.Dropout, [0.1]]\n  - [-1, 1, Conv, [256, 3, 2]]\n  - [[-1, 12], 1, Concat, [1]]\n  - [-1, 3, C2f, [512]]\n  - [-1, 1, nn.Dropout, [0.1]]\n  - [-1, 1, Conv, [512, 3, 2]]\n  - [[-1, 9], 1, Concat, [1]]\n  - [-1, 3, C2f, [1024]]\n  - [-1, 1, nn.Dropout, [0.1]]\n  - [[16, 20, 24], 1, Detect, [nc]]\n\"\"\"\nCUSTOM_MODEL_YAML_PATH = '/kaggle/working/yolov8l_dropout.yaml'\nwith open(CUSTOM_MODEL_YAML_PATH, 'w') as f: f.write(CUSTOM_MODEL_YAML_CONTENT)\nprint(f\"✅ Created STABLE custom model config at: {CUSTOM_MODEL_YAML_PATH}\")\n\nprint(\"\\n\\n\\n---\")\nprint(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\nprint(\"!!! CRITICAL: SETUP COMPLETE. Please restart the kernel NOW.           !!!\")\nprint(\"!!! Use the 'Run' menu -> 'Restart Session' or the circular arrow icon.!!!\")\nprint(\"!!! After restarting, DO NOT run this cell again. Run Cell 2 directly. !!!\")\nprint(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.014868Z","iopub.status.idle":"2025-06-17T16:44:59.015733Z","shell.execute_reply.started":"2025-06-17T16:44:59.015564Z","shell.execute_reply":"2025-06-17T16:44:59.015580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CODE CELL 2: TRAINING & ANALYSIS\n\n# --- 1. IMPORTS ---\n# This will now work because the kernel was restarted after installation.\nimport cv2\nimport torch\nimport numpy as np\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon\nprint(\"✅ Imports successful. The ultralytics library was found.\")\n\n\n# --- 2. TRAINING ---\nprint(\"\\n--- Starting Model Training ---\")\nCUSTOM_MODEL_YAML_PATH = '/kaggle/working/yolov8l_dropout.yaml'\nDATA_YAML_PATH = '/kaggle/working/vehicle_data.yaml'\n\nmodel = YOLO(CUSTOM_MODEL_YAML_PATH)\nmodel.train(\n    data=DATA_YAML_PATH,\n    pretrained='yolov8l.pt',\n    imgsz=640,\n    batch=8,\n    epochs=300,\n    name='yolov8l_dropout_run',\n    project='/kaggle/working/runs/train',\n    cache=True,\n    patience=30,\n)\nprint(f\"\\n--- Training Complete ---\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.016468Z","iopub.status.idle":"2025-06-17T16:44:59.016794Z","shell.execute_reply.started":"2025-06-17T16:44:59.016642Z","shell.execute_reply":"2025-06-17T16:44:59.016657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. ANALYSIS ---\nprint(\"\\n--- Starting Video Analysis ---\")\n\n# Import necessary libraries if not already imported\nimport torch\nimport cv2\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon\n\nBEST_MODEL_PATH = '/kaggle/working/runs/train/yolov8l_dropout_run/weights/best.pt'\nFINAL_CLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle']\nVIDEO_PATH = '/kaggle/input/videohai/traffic.mp4'\n\n# --- FIX 2: Correct the output path to the writable /kaggle/working/ directory ---\nOUTPUT_VIDEO_PATH = '/kaggle/working/traffic_analysis_output.mp4'\n\nLANE_LINES = [[(100,720),(500,450)],[(550,720),(650,450)],[(800,720),(850,450)],[(1200,720),(1100,450)]]\nCONF_THRESHOLD = 0.4\n\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\nmodel=YOLO(BEST_MODEL_PATH); model.to(device)\ntracker=DeepSort(max_age=30,nn_budget=50,n_init=3)\ncap=cv2.VideoCapture(VIDEO_PATH)\n\nif not cap.isOpened(): \n    print(f\"Error opening video {VIDEO_PATH}\")\nelse:\n    w,h,fps=(int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n    out=cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w,h))\n    \n    # Setup lane polygons and data structures\n    lane_polygons=[Polygon(LANE_LINES[i]+LANE_LINES[i+1][::-1]) for i in range(len(LANE_LINES)-1)]\n    lane_vehicle_ids={i+1:{cls:set() for cls in FINAL_CLASS_NAMES} for i in range(len(lane_polygons))}\n    \n    frame_num=0\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    print(f\"Analyzing video with {total_frames} frames...\")\n    \n    while cap.isOpened():\n        ret,frame=cap.read()\n        if not ret:break\n            \n        if frame_num % 100 == 0:\n            print(f\"Processing frame {frame_num}/{total_frames}...\")\n        frame_num+=1\n        \n        # Get detections from YOLOv8\n        results=model(frame, verbose=False)\n        \n        # Prepare detections for DeepSort in [l,t,w,h] format\n        detections=[]\n        for box in results[0].boxes:\n            if box.conf[0] > CONF_THRESHOLD:\n                x1,y1,x2,y2 = box.xyxy[0].cpu().numpy()\n                w_box, h_box = x2 - x1, y2 - y1\n                detections.append(([int(x1), int(y1), int(w_box), int(h_box)], box.conf[0], int(box.cls[0])))\n        \n        # Update the tracker\n        tracks=tracker.update_tracks(detections, frame=frame)\n        \n        # Process each track\n        for track in tracks:\n            if not track.is_confirmed() or track.time_since_update > 2:\n                continue\n                \n            # --- FIX 1: Removed the 'orig_image_wh' argument from to_ltrb() ---\n            track_id, bbox, cls_id = track.track_id, track.to_ltrb(), track.get_det_class()\n            \n            class_name = FINAL_CLASS_NAMES[cls_id]\n            \n            # Check which lane the object is in\n            # Using the bottom-center point of the bounding box for lane assignment\n            bottom_center_point = Point((bbox[0] + bbox[2]) / 2, bbox[3])\n            assigned_lane = next((i + 1 for i, poly in enumerate(lane_polygons) if poly.contains(bottom_center_point)), None)\n            \n            if assigned_lane:\n                lane_vehicle_ids[assigned_lane][class_name].add(track_id)\n            \n            # Draw bounding box and label\n            label = f\"{class_name} {track_id}\" + (f\" L:{assigned_lane}\" if assigned_lane else \"\")\n            cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n            \n        # Draw lane counts on the frame\n        y_offset = 40\n        for lane_num, class_counts in lane_vehicle_ids.items():\n            total_in_lane = sum(len(ids) for ids in class_counts.values())\n            cv2.putText(frame, f\"Lane {lane_num} Total: {total_in_lane}\", (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)\n            y_offset += 30\n            breakdown = \", \".join([f\"{name}:{len(ids)}\" for name, ids in class_counts.items() if len(ids) > 0])\n            cv2.putText(frame, breakdown, (30, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)\n            y_offset += 40\n            \n        out.write(frame)\n        \n    cap.release()\n    out.release()\n    \n    print(f\"\\n--- Analysis Complete ---\")\n    print(f\"Output video saved to: {OUTPUT_VIDEO_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.018037Z","iopub.status.idle":"2025-06-17T16:44:59.018332Z","shell.execute_reply.started":"2025-06-17T16:44:59.018182Z","shell.execute_reply":"2025-06-17T16:44:59.018195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- IMPORTS ---\nimport torch\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon\nimport time\nfrom collections import defaultdict\n\n# --- CONFIGURATION ---\nprint(\"\\n--- Initializing Configuration ---\")\n\n# Model and Video Paths\nBEST_MODEL_PATH = '/kaggle/working/runs/train/yolov8l_dropout_run/weights/best.pt'\nVIDEO_PATH = '/kaggle/input/videohai/traffic.mp4'\nOUTPUT_VIDEO_PATH = '/kaggle/working/traffic_analysis_stable.mp4'\n\n# --- DETECTION & TRACKING PARAMETERS (FOR STABILITY) ---\nTARGET_CLASSES = [0, 1, 2, 3] # 'car', 'truck', 'bus', 'motorcycle'\nCLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle']\n\n# STABILITY FIX 1: Lower the confidence threshold to allow weaker detections to be processed by the tracker.\n# The tracker's logic is better at filtering noise than a simple confidence score.\nCONF_THRESHOLD = 0.3\nIOU_THRESHOLD = 0.5\n\n# STABILITY FIX 2: Tune DeepSORT parameters for more robust and persistent tracking.\nDEEPSORT_CONFIG = {\n    'max_age': 50,          # The most important parameter. Increase this to keep tracks alive longer when they are not detected.\n    'n_init': 5,            # Number of consecutive detections to confirm a track. Helps prevent false positives.\n    'nms_max_overlap': 1.0, # NMS threshold for DeepSORT.\n    'nn_budget': 100        # Size of the appearance gallery. A larger budget helps with long-term re-identification.\n}\n\n# PERFORMANCE: Batch size for model inference.\nBATCH_SIZE = 4\n\n# Analysis Parameters\nLANE_LINE_DEFINITIONS = [\n    [(100, 720), (500, 450)],  # Lane 1 Left\n    [(550, 720), (650, 450)],  # Lane 1 Right / Lane 2 Left\n    [(800, 720), (850, 450)],  # Lane 2 Right / Lane 3 Left\n    [(1200, 720), (1100, 450)] # Lane 3 Right\n]\nCONGESTION_THRESHOLD = 10\n\n\n# --- INITIALIZATION ---\nprint(\"--- Initializing Model and Tracker ---\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\nmodel = YOLO(BEST_MODEL_PATH)\nmodel.to(device)\n\n# Initialize DeepSort tracker with our new stable configuration\ntracker = DeepSort(\n    max_age=DEEPSORT_CONFIG['max_age'],\n    n_init=DEEPSORT_CONFIG['n_init'],\n    nms_max_overlap=DEEPSORT_CONFIG['nms_max_overlap'],\n    nn_budget=DEEPSORT_CONFIG['nn_budget']\n)\n\n# Video I/O\ncap = cv2.VideoCapture(VIDEO_PATH)\nif not cap.isOpened(): raise IOError(f\"Error opening video: {VIDEO_PATH}\")\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nout = cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n\n# Lane Polygons and Data Structures\nlane_polygons = [Polygon(LANE_LINE_DEFINITIONS[i] + LANE_LINE_DEFINITIONS[i+1][::-1]) for i in range(len(LANE_LINE_DEFINITIONS) - 1)]\ntotal_vehicle_counts_by_lane = {i+1: defaultdict(set) for i in range(len(lane_polygons))}\n\n\n# --- MAIN PROCESSING LOOP ---\nprint(f\"--- Starting Stable Video Analysis ({total_frames} frames) ---\")\nframes_batch = []\nframe_num = 0\nstart_time = time.time()\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    frames_batch.append(frame)\n    frame_num += 1\n\n    if len(frames_batch) == BATCH_SIZE or frame_num == total_frames:\n        results_batch = model.predict(frames_batch, classes=TARGET_CLASSES, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD, verbose=False)\n\n        for i, results in enumerate(results_batch):\n            current_frame = frames_batch[i]\n            \n            detections = []\n            for box in results.boxes:\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                w_box, h_box = x2 - x1, y2 - y1\n                detections.append(([int(x1), int(y1), int(w_box), int(h_box)], box.conf[0].item(), CLASS_NAMES[int(box.cls[0].item())]))\n            \n            tracks = tracker.update_tracks(detections, frame=current_frame)\n            current_lane_counts = {i+1: 0 for i in range(len(lane_polygons))}\n            \n            for track in tracks:\n                # We only process CONFIRMED tracks that have been recently updated.\n                if not track.is_confirmed() or track.time_since_update > 1:\n                    continue\n                \n                track_id, bbox, class_name = track.track_id, track.to_ltrb(), track.get_det_class()\n                bottom_center_point = Point((bbox[0] + bbox[2]) / 2, bbox[3])\n                assigned_lane = next((i + 1 for i, poly in enumerate(lane_polygons) if poly.contains(bottom_center_point)), None)\n                \n                if assigned_lane:\n                    current_lane_counts[assigned_lane] += 1\n                    total_vehicle_counts_by_lane[assigned_lane][class_name].add(track_id)\n                \n                label = f\"{class_name} {track_id}\" + (f\" L:{assigned_lane}\" if assigned_lane else \"\")\n                cv2.rectangle(current_frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n                cv2.putText(current_frame, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n            y_offset = 40\n            for lane_num, poly in enumerate(lane_polygons, 1):\n                is_congested = current_lane_counts[lane_num] > CONGESTION_THRESHOLD\n                lane_color = (0, 0, 255) if is_congested else (0, 255, 0)\n                poly_pts = np.array(poly.exterior.coords, np.int32).reshape((-1, 1, 2))\n                cv2.polylines(current_frame, [poly_pts], isClosed=True, color=lane_color, thickness=2)\n                status = \"Congested\" if is_congested else \"Normal\"\n                info_text = f\"Lane {lane_num} ({status}): {current_lane_counts[lane_num]} vehicles\"\n                cv2.putText(current_frame, info_text, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n                y_offset += 40\n\n            out.write(current_frame)\n\n        frames_batch = []\n        elapsed_time = time.time() - start_time\n        processed_frames = frame_num - len(frames_batch)\n        fps_current = processed_frames / elapsed_time if elapsed_time > 0 else 0\n        print(f\"Processed frame {processed_frames}/{total_frames}... Current FPS: {fps_current:.2f}\")\n\n\n# --- CLEANUP AND REPORT ---\ncap.release()\nout.release()\nprint(f\"\\n--- Analysis Complete ---\")\nprint(f\"Output video saved to: {OUTPUT_VIDEO_PATH}\")\n\nprint(\"\\n--- Final Vehicle Count Report (Unique Vehicles per Lane) ---\")\nfor lane_num, class_counts in total_vehicle_counts_by_lane.items():\n    total_in_lane = sum(len(ids) for ids in class_counts.values())\n    print(f\"\\nLane {lane_num} | Total Unique Vehicles: {total_in_lane}\")\n    breakdown = \", \".join([f\"{name}: {len(ids)}\" for name, ids in class_counts.items() if len(ids) > 0])\n    print(f\"  > Breakdown: {breakdown}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.019446Z","iopub.status.idle":"2025-06-17T16:44:59.019766Z","shell.execute_reply.started":"2025-06-17T16:44:59.019614Z","shell.execute_reply":"2025-06-17T16:44:59.019628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- IMPORTS ---\nimport torch\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon\nimport time\nfrom collections import defaultdict, deque\nimport pandas as pd\n\n# --- CONFIGURATION ---\nprint(\"\\n--- Initializing Configuration ---\")\n\n# --- CORE PARAMETERS ---\nMODEL_PATH = '/kaggle/working/runs/train/yolov8l_dropout_run/weights/best.pt'\nVIDEO_PATH = '/kaggle/input/videohai/traffic.mp4'\nOUTPUT_VIDEO_PATH = '/kaggle/working/traffic_intelligence_output.mp4'\nCSV_LOG_PATH = '/kaggle/working/traffic_events_log.csv'\n\n# --- DETECTION & TRACKING ---\nTARGET_CLASSES = [0, 1, 2, 3] # 'car', 'truck', 'bus', 'motorcycle'\nCLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle']\nCONF_THRESHOLD = 0.3\nIOU_THRESHOLD = 0.5\nDEEPSORT_CONFIG = {'max_age': 50, 'n_init': 5, 'nms_max_overlap': 1.0, 'nn_budget': 100}\nBATCH_SIZE = 4 # Adjust based on VRAM\n\n# --- ANALYSIS & VISUALIZATION ---\n# Lane definitions\nLANE_LINE_DEFINITIONS = [\n    [(100, 720), (500, 450)],  # Lane 1 Left\n    [(550, 720), (650, 450)],  # Lane 1 Right / Lane 2 Left\n    [(800, 720), (850, 450)],  # Lane 2 Right / Lane 3 Left\n    [(1200, 720), (1100, 450)] # Lane 3 Right\n]\n# **IMPORTANT**: Define the real-world length of the lanes in METERS for speed estimation.\n# Measure the approximate vertical distance covered by the lanes in your video frame.\nREAL_WORLD_LANE_LENGTH_METERS = 30 \n# Define the expected flow of traffic ('up' or 'down' relative to the Y-axis)\nLANE_FLOW_DIRECTION = 'up' # Vehicles move from high Y to low Y\n# Thresholds for advanced analysis\nCONGESTION_THRESHOLD = 10  # Vehicles per lane\nSTOPPED_VEHICLE_SPEED_THRESHOLD_KMPH = 3 # Speed below which a vehicle is considered stopped\nSTOPPED_VEHICLE_DURATION_SECONDS = 5 # How long a vehicle must be stopped to be flagged\nSPEED_UNIT = 'km/h' # or 'mph'\n\n# --- INITIALIZATION ---\nprint(\"--- Initializing Model, Tracker, and Video I/O ---\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = YOLO(MODEL_PATH)\nmodel.to(device)\ntracker = DeepSort(max_age=DEEPSORT_CONFIG['max_age'], n_init=DEEPSORT_CONFIG['n_init'], nms_max_overlap=DEEPSORT_CONFIG['nms_max_overlap'], nn_budget=DEEPSORT_CONFIG['nn_budget'])\ncap = cv2.VideoCapture(VIDEO_PATH)\nif not cap.isOpened(): raise IOError(f\"Error opening video: {VIDEO_PATH}\")\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nout = cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n\n# --- DATA STRUCTURES ---\nlane_polygons = [Polygon(LANE_LINE_DEFINITIONS[i] + LANE_LINE_DEFINITIONS[i+1][::-1]) for i in range(len(LANE_LINE_DEFINITIONS) - 1)]\ntrack_history = defaultdict(lambda: deque(maxlen=int(fps))) # Store position history for speed calculation\nstopped_timers = defaultdict(int)\nevent_log = []\n\n# --- MAIN PROCESSING LOOP ---\nprint(f\"--- Starting Advanced Video Analysis ({total_frames} frames) ---\")\nframes_batch = []\nframe_num = 0\nstart_time = time.time()\n\ntry:\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret: break\n        frames_batch.append(frame)\n        frame_num += 1\n\n        if len(frames_batch) == BATCH_SIZE or frame_num == total_frames:\n            results_batch = model.predict(frames_batch, classes=TARGET_CLASSES, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD, verbose=False)\n\n            for i, results in enumerate(results_batch):\n                current_frame_index = frame_num - len(frames_batch) + i\n                current_frame_time = current_frame_index / fps\n                current_frame = frames_batch[i]\n                \n                detections = []\n                for box in results.boxes:\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    detections.append(([int(x1), int(y1), int(x2 - x1), int(y2 - y1)], box.conf[0].item(), CLASS_NAMES[int(box.cls[0].item())]))\n                \n                tracks = tracker.update_tracks(detections, frame=current_frame)\n                current_lane_counts = {lane: 0 for lane in range(1, len(lane_polygons) + 1)}\n                \n                dashboard = np.zeros_like(current_frame, dtype=np.uint8)\n\n                for track in tracks:\n                    if not track.is_confirmed() or track.time_since_update > 1: continue\n                    \n                    track_id, bbox, class_name = track.track_id, track.to_ltrb(), track.get_det_class()\n                    bottom_center = Point((bbox[0] + bbox[2]) / 2, bbox[3])\n                    \n                    # Update history and calculate speed\n                    track_history[track_id].append(bottom_center)\n                    speed_kmph = -1\n                    if len(track_history[track_id]) > int(fps / 2): # Need at least half a second of history\n                        prev_pos = track_history[track_id][0]\n                        pixel_dist = bottom_center.distance(prev_pos)\n                        time_diff = len(track_history[track_id]) / fps\n                        \n                        # Perspective correction for speed\n                        perspective_scale = 1 - (bottom_center.y / h) # Closer objects (higher y) move more pixels\n                        real_world_dist = pixel_dist * perspective_scale * (REAL_WORLD_LANE_LENGTH_METERS / h)\n                        speed_mps = real_world_dist / time_diff\n                        speed_kmph = speed_mps * (3.6 if SPEED_UNIT == 'km/h' else 2.23694)\n                    \n                    assigned_lane = next((i + 1 for i, poly in enumerate(lane_polygons) if poly.contains(bottom_center)), None)\n                    if assigned_lane: current_lane_counts[assigned_lane] += 1\n\n                    # --- ADVANCED ANALYSIS CHECKS ---\n                    is_stopped = False\n                    is_wrong_way = False\n\n                    # Stopped vehicle check\n                    if speed_kmph != -1 and speed_kmph < STOPPED_VEHICLE_SPEED_THRESHOLD_KMPH:\n                        stopped_timers[track_id] += 1\n                        if (stopped_timers[track_id] / fps) > STOPPED_VEHICLE_DURATION_SECONDS:\n                            is_stopped = True\n                            event_log.append({'timestamp': current_frame_time, 'event': 'stopped_vehicle', 'track_id': track_id, 'lane': assigned_lane, 'speed': speed_kmph})\n                    else:\n                        stopped_timers[track_id] = 0\n\n                    # Wrong-way check\n                    if len(track_history[track_id]) > 2:\n                        direction_y = bottom_center.y - track_history[track_id][-2].y\n                        flow = 'up' if direction_y < 0 else 'down'\n                        if assigned_lane and flow != LANE_FLOW_DIRECTION:\n                            is_wrong_way = True\n                            event_log.append({'timestamp': current_frame_time, 'event': 'wrong_way', 'track_id': track_id, 'lane': assigned_lane})\n\n                    # --- VISUALIZATION ---\n                    label = f\"{class_name} {track_id}\"\n                    color = (0, 255, 0) # Default green\n                    if is_stopped:\n                        label += \" [STOPPED]\"\n                        color = (0, 0, 255) # Red for stopped\n                    elif is_wrong_way:\n                        label += \" [WRONG WAY]\"\n                        color = (0, 255, 255) # Yellow for wrong way\n                    elif speed_kmph > 0:\n                        label += f\" {int(speed_kmph)} {SPEED_UNIT}\"\n\n                    cv2.rectangle(current_frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n                    cv2.putText(current_frame, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n                \n                # Draw Dashboard and Lane Polygons\n                y_offset = 40\n                for lane_num, poly in enumerate(lane_polygons, 1):\n                    is_congested = current_lane_counts[lane_num] > CONGESTION_THRESHOLD\n                    lane_color = (0, 0, 255) if is_congested else (0, 255, 0)\n                    poly_pts = np.array(poly.exterior.coords, np.int32).reshape((-1, 1, 2))\n                    cv2.polylines(current_frame, [poly_pts], isClosed=True, color=lane_color, thickness=2)\n                    status = \"Congested\" if is_congested else \"Normal\"\n                    info_text = f\"Lane {lane_num} ({status}): {current_lane_counts[lane_num]} veh\"\n                    cv2.putText(dashboard, info_text, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n                    y_offset += 30\n                    if is_congested and not any(e['event'] == 'congestion' and e['lane'] == lane_num and abs(e['timestamp'] - current_frame_time) < 1 for e in event_log):\n                        event_log.append({'timestamp': current_frame_time, 'event': 'congestion', 'lane': lane_num, 'vehicle_count': current_lane_counts[lane_num]})\n\n                # Combine frame with dashboard\n                final_frame = cv2.addWeighted(current_frame, 1, dashboard, 0.8, 0)\n                out.write(final_frame)\n\n            frames_batch = []\n            elapsed_time = time.time() - start_time\n            processed_frames = frame_num - len(frames_batch)\n            fps_current = processed_frames / elapsed_time if elapsed_time > 0 else 0\n            print(f\"Processed frame {processed_frames}/{total_frames}... Current FPS: {fps_current:.2f}\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    # --- CLEANUP AND REPORT ---\n    cap.release()\n    out.release()\n    print(f\"\\n--- Analysis Complete ---\")\n    print(f\"Output video saved to: {OUTPUT_VIDEO_PATH}\")\n\n    # --- DATA EXPORT TO CSV ---\n    if event_log:\n        df = pd.DataFrame(event_log)\n        df.to_csv(CSV_LOG_PATH, index=False)\n        print(f\"Event log saved to: {CSV_LOG_PATH}\")\n    else:\n        print(\"No critical events were logged.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.020947Z","iopub.status.idle":"2025-06-17T16:44:59.021155Z","shell.execute_reply.started":"2025-06-17T16:44:59.021058Z","shell.execute_reply":"2025-06-17T16:44:59.021067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- IMPORTS ---\nimport torch\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon, LineString\nimport time\nfrom collections import defaultdict, deque\nimport pandas as pd\n\n# --- CONFIGURATION ---\nprint(\"\\n--- Initializing Configuration ---\")\n\n# --- CORE PARAMETERS ---\nMODEL_PATH = '/kaggle/working/runs/train/yolov8l_dropout_run/weights/best.pt'\nVIDEO_PATH = '/kaggle/input/videohai/traffic.mp4'\nOUTPUT_VIDEO_PATH = '/kaggle/working/highway_intelligence_final.mp4'\nCSV_LOG_PATH = '/kaggle/working/highway_events_log_final.csv'\n\n# --- DETECTION & TRACKING ---\nTARGET_CLASSES = [0, 1, 2, 3]\nCLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle']\nCONF_THRESHOLD = 0.3\nIOU_THRESHOLD = 0.5\nDEEPSORT_CONFIG = {'max_age': 50, 'n_init': 5, 'nms_max_overlap': 1.0, 'nn_budget': 100}\nBATCH_SIZE = 4\n\n# --- ANALYSIS & VISUALIZATION ---\nLANE_LINE_DEFINITIONS = [\n    [(100, 720), (500, 450)],\n    [(550, 720), (650, 450)],\n    [(800, 720), (850, 450)],\n    [(1200, 720), (1100, 450)]\n]\nCOUNTING_LINE_Y_POSITION = 650\nDENSITY_THRESHOLDS = {'normal': 0.15, 'medium': 0.30}\n\n# --- HIGHWAY SPEED ANALYSIS CONFIGURATION ---\nREAL_WORLD_LANE_LENGTH_METERS = 30 \nSPEED_CALIBRATION_FACTOR = 3.5 \nSPEED_LIMIT_KMPH = 100\nSPEED_UNIT = 'km/h'\nSTOPPED_VEHICLE_SPEED_THRESHOLD_KMPH = 5\nSTOPPED_VEHICLE_DURATION_SECONDS = 2\nDASHBOARD_TRANSPARENCY = 0.6\n\n# --- INITIALIZATION ---\nprint(\"--- Initializing Model, Tracker, and Video I/O ---\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = YOLO(MODEL_PATH)\nmodel.to(device)\ntracker = DeepSort(max_age=DEEPSORT_CONFIG['max_age'], n_init=DEEPSORT_CONFIG['n_init'], nms_max_overlap=DEEPSORT_CONFIG['nms_max_overlap'], nn_budget=DEEPSORT_CONFIG['nn_budget'])\ncap = cv2.VideoCapture(VIDEO_PATH)\nif not cap.isOpened(): raise IOError(f\"Error opening video: {VIDEO_PATH}\")\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nout = cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n\n# --- DATA STRUCTURES ---\nlane_polygons = [Polygon(LANE_LINE_DEFINITIONS[i] + LANE_LINE_DEFINITIONS[i+1][::-1]) for i in range(len(LANE_LINE_DEFINITIONS) - 1)]\nlane_areas_pixels = [poly.area for poly in lane_polygons]\ntrack_history = defaultdict(lambda: deque(maxlen=int(fps)))\nstopped_timers = defaultdict(int)\ncumulative_counts_by_lane = defaultdict(int)\ncounted_ids_by_lane = defaultdict(set)\ncounting_line = LineString([(0, COUNTING_LINE_Y_POSITION), (w, COUNTING_LINE_Y_POSITION)])\nevent_log = []\n\n# --- MAIN PROCESSING LOOP ---\nprint(f\"--- Starting Highway Analysis ({total_frames} frames) ---\")\nframes_batch = []\nframe_num = 0\nstart_time = time.time()\n\ntry:\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret: break\n        frames_batch.append(frame)\n        frame_num += 1\n\n        if len(frames_batch) == BATCH_SIZE or frame_num == total_frames:\n            results_batch = model.predict(frames_batch, classes=TARGET_CLASSES, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD, verbose=False)\n\n            for i, results in enumerate(results_batch):\n                current_frame = frames_batch[i]\n                \n                detections = []\n                for box in results.boxes:\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    detections.append(([int(x1), int(y1), int(x2 - x1), int(y2 - y1)], box.conf[0].item(), CLASS_NAMES[int(box.cls[0].item())]))\n                \n                tracks = tracker.update_tracks(detections, frame=current_frame)\n                current_lane_counts = defaultdict(int)\n                lane_vehicle_areas = defaultdict(float)\n\n                for track in tracks:\n                    if not track.is_confirmed() or track.time_since_update > 1: continue\n                    \n                    track_id, bbox, class_name = track.track_id, track.to_ltrb(), track.get_det_class()\n                    \n                    # --- BUG FIX: Initialize speed_kmph to a default value for every track ---\n                    speed_kmph = -1\n                    \n                    bottom_center = Point((bbox[0] + bbox[2]) / 2, bbox[3])\n                    track_history[track_id].append(bottom_center)\n                    \n                    if len(track_history[track_id]) > int(fps / 2):\n                        prev_pos = track_history[track_id][0]\n                        pixel_dist = bottom_center.distance(prev_pos)\n                        time_diff = len(track_history[track_id]) / fps\n                        perspective_scale = 1 - (bottom_center.y / h)\n                        real_world_dist = pixel_dist * perspective_scale * (REAL_WORLD_LANE_LENGTH_METERS / h)\n                        speed_mps = real_world_dist / time_diff if time_diff > 0 else 0\n                        speed_kmph = speed_mps * 3.6 * SPEED_CALIBRATION_FACTOR\n\n                    is_stopped = speed_kmph < STOPPED_VEHICLE_SPEED_THRESHOLD_KMPH and speed_kmph != -1\n                    is_speeding = speed_kmph > SPEED_LIMIT_KMPH\n\n                    # ... (Rest of the analysis and visualization logic) ...\n                    assigned_lane = next((i + 1 for i, poly in enumerate(lane_polygons) if poly.contains(bottom_center)), None)\n                    if assigned_lane:\n                        current_lane_counts[assigned_lane] += 1\n                        bbox_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n                        lane_vehicle_areas[assigned_lane] += bbox_area\n                    if len(track_history[track_id]) > 1 and assigned_lane is not None:\n                        if LineString([track_history[track_id][-2], track_history[track_id][-1]]).intersects(counting_line):\n                            if track_id not in counted_ids_by_lane[assigned_lane]:\n                                cumulative_counts_by_lane[assigned_lane] += 1\n                                counted_ids_by_lane[assigned_lane].add(track_id)\n\n                    if is_speeding: label, color = f\"{class_name} {track_id} [{int(speed_kmph)} {SPEED_UNIT}] SPEEDING\", (255, 0, 255)\n                    elif is_stopped:\n                        stopped_timers[track_id] += 1\n                        if (stopped_timers[track_id] / fps) > STOPPED_VEHICLE_DURATION_SECONDS: label, color = f\"{class_name} {track_id} [STOPPED]\", (0, 0, 255)\n                        else: label, color = f\"{class_name} {track_id} {int(speed_kmph)} {SPEED_UNIT}\", (0, 255, 0)\n                    else:\n                        stopped_timers[track_id] = 0\n                        label = f\"{class_name} {track_id} {int(speed_kmph)} {SPEED_UNIT}\" if speed_kmph > 0 else f\"{class_name} {track_id}\"\n                        color = (0, 255, 0)\n                    cv2.rectangle(current_frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n                    cv2.putText(current_frame, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n                # --- Dashboard drawing logic remains the same ---\n                dashboard_panel = np.zeros((200, w, 3), dtype=np.uint8)\n                y_offset = 30\n                for lane_num, poly in enumerate(lane_polygons, 1):\n                    density_ratio = lane_vehicle_areas[lane_num] / lane_areas_pixels[lane_num - 1] if lane_areas_pixels[lane_num - 1] > 0 else 0\n                    if density_ratio <= DENSITY_THRESHOLDS['normal']: density_level = \"Normal\"\n                    elif density_ratio <= DENSITY_THRESHOLDS['medium']: density_level = \"Medium\"\n                    else: density_level = \"High\"\n                    lane_color = (0, 165, 255) if density_level == \"Medium\" else (0, 0, 255) if density_level == \"High\" else (0, 255, 0)\n                    poly_pts = np.array(poly.exterior.coords, np.int32).reshape((-1, 1, 2))\n                    cv2.polylines(current_frame, [poly_pts], isClosed=True, color=lane_color, thickness=2)\n                    info_text = f\"Lane {lane_num} | Density: {density_level} ({density_ratio:.1%}) | Live Count: {current_lane_counts[lane_num]}\"\n                    cv2.putText(dashboard_panel, info_text, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n                    y_offset += 30\n                cv2.putText(dashboard_panel, \"Cumulative Vehicle Count (Total Passed)\", (20, y_offset + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n                y_offset += 50\n                count_text = \"  \".join([f\"Lane {ln}: {cnt}\" for ln, cnt in sorted(cumulative_counts_by_lane.items())])\n                cv2.putText(dashboard_panel, count_text, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n                roi = current_frame[0:200, 0:w]\n                blended_roi = cv2.addWeighted(roi, 1.0, dashboard_panel, DASHBOARD_TRANSPARENCY, 0)\n                current_frame[0:200, 0:w] = blended_roi\n                cv2.line(current_frame, (0, COUNTING_LINE_Y_POSITION), (w, COUNTING_LINE_Y_POSITION), (255, 255, 0), 2)\n                out.write(current_frame)\n\n            frames_batch = []\n            elapsed_time = time.time() - start_time\n            processed_frames = frame_num - len(frames_batch)\n            fps_current = processed_frames / elapsed_time if elapsed_time > 0 else 0\n            print(f\"Processed frame {processed_frames}/{total_frames}... Current FPS: {fps_current:.2f}\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    cap.release()\n    out.release()\n    print(f\"\\n--- Analysis Complete ---\")\n    print(f\"Output video saved to: {OUTPUT_VIDEO_PATH}\")\n    if event_log:\n        df = pd.DataFrame(event_log)\n        df.to_csv(CSV_LOG_PATH, index=False)\n        print(f\"Event log saved to: {CSV_LOG_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T17:02:11.989799Z","iopub.execute_input":"2025-06-17T17:02:11.990089Z","iopub.status.idle":"2025-06-17T17:03:26.713833Z","shell.execute_reply.started":"2025-06-17T17:02:11.990070Z","shell.execute_reply":"2025-06-17T17:03:26.712815Z"}},"outputs":[{"name":"stdout","text":"\n--- Initializing Configuration ---\n--- Initializing Model, Tracker, and Video I/O ---\n--- Starting Highway Analysis (900 frames) ---\nProcessed frame 4/900... Current FPS: 3.63\nProcessed frame 8/900... Current FPS: 5.79\nProcessed frame 12/900... Current FPS: 7.19\nProcessed frame 16/900... Current FPS: 8.12\nProcessed frame 20/900... Current FPS: 8.82\nProcessed frame 24/900... Current FPS: 9.36\nProcessed frame 28/900... Current FPS: 9.76\nProcessed frame 32/900... Current FPS: 10.08\nProcessed frame 36/900... Current FPS: 10.32\nProcessed frame 40/900... Current FPS: 10.52\nProcessed frame 44/900... Current FPS: 10.69\nProcessed frame 48/900... Current FPS: 10.86\nProcessed frame 52/900... Current FPS: 10.98\nProcessed frame 56/900... Current FPS: 11.07\nProcessed frame 60/900... Current FPS: 11.15\nProcessed frame 64/900... Current FPS: 11.31\nProcessed frame 68/900... Current FPS: 11.38\nProcessed frame 72/900... Current FPS: 11.45\nProcessed frame 76/900... Current FPS: 11.51\nProcessed frame 80/900... Current FPS: 11.57\nProcessed frame 84/900... Current FPS: 11.63\nProcessed frame 88/900... Current FPS: 11.68\nProcessed frame 92/900... Current FPS: 11.72\nProcessed frame 96/900... Current FPS: 11.76\nProcessed frame 100/900... Current FPS: 11.77\nProcessed frame 104/900... Current FPS: 11.79\nProcessed frame 108/900... Current FPS: 11.80\nProcessed frame 112/900... Current FPS: 11.78\nProcessed frame 116/900... Current FPS: 11.79\nProcessed frame 120/900... Current FPS: 11.80\nProcessed frame 124/900... Current FPS: 11.81\nProcessed frame 128/900... Current FPS: 11.82\nProcessed frame 132/900... Current FPS: 11.82\nProcessed frame 136/900... Current FPS: 11.81\nProcessed frame 140/900... Current FPS: 11.78\nProcessed frame 144/900... Current FPS: 11.77\nProcessed frame 148/900... Current FPS: 11.76\nProcessed frame 152/900... Current FPS: 11.76\nProcessed frame 156/900... Current FPS: 11.76\nProcessed frame 160/900... Current FPS: 11.76\nProcessed frame 164/900... Current FPS: 11.75\nProcessed frame 168/900... Current FPS: 11.75\nProcessed frame 172/900... Current FPS: 11.74\nProcessed frame 176/900... Current FPS: 11.73\nProcessed frame 180/900... Current FPS: 11.71\nProcessed frame 184/900... Current FPS: 11.70\nProcessed frame 188/900... Current FPS: 11.68\nProcessed frame 192/900... Current FPS: 11.66\nProcessed frame 196/900... Current FPS: 11.65\nProcessed frame 200/900... Current FPS: 11.62\nProcessed frame 204/900... Current FPS: 11.60\nProcessed frame 208/900... Current FPS: 11.57\nProcessed frame 212/900... Current FPS: 11.52\nProcessed frame 216/900... Current FPS: 11.47\nProcessed frame 220/900... Current FPS: 11.44\nProcessed frame 224/900... Current FPS: 11.41\nProcessed frame 228/900... Current FPS: 11.37\nProcessed frame 232/900... Current FPS: 11.34\nProcessed frame 236/900... Current FPS: 11.31\nProcessed frame 240/900... Current FPS: 11.23\nProcessed frame 244/900... Current FPS: 11.16\nProcessed frame 248/900... Current FPS: 11.11\nProcessed frame 252/900... Current FPS: 11.07\nProcessed frame 256/900... Current FPS: 11.03\nProcessed frame 260/900... Current FPS: 11.03\nProcessed frame 264/900... Current FPS: 10.99\nProcessed frame 268/900... Current FPS: 10.96\nProcessed frame 272/900... Current FPS: 10.94\nProcessed frame 276/900... Current FPS: 10.94\nProcessed frame 280/900... Current FPS: 10.94\nProcessed frame 284/900... Current FPS: 10.93\nProcessed frame 288/900... Current FPS: 10.93\nProcessed frame 292/900... Current FPS: 10.93\nProcessed frame 296/900... Current FPS: 10.94\nProcessed frame 300/900... Current FPS: 10.94\nProcessed frame 304/900... Current FPS: 10.94\nProcessed frame 308/900... Current FPS: 10.94\nProcessed frame 312/900... Current FPS: 10.94\nProcessed frame 316/900... Current FPS: 10.94\nProcessed frame 320/900... Current FPS: 10.95\nProcessed frame 324/900... Current FPS: 10.94\nProcessed frame 328/900... Current FPS: 10.95\nProcessed frame 332/900... Current FPS: 10.96\nProcessed frame 336/900... Current FPS: 10.96\nProcessed frame 340/900... Current FPS: 10.96\nProcessed frame 344/900... Current FPS: 10.97\nProcessed frame 348/900... Current FPS: 10.97\nProcessed frame 352/900... Current FPS: 10.98\nProcessed frame 356/900... Current FPS: 10.99\nProcessed frame 360/900... Current FPS: 10.99\nProcessed frame 364/900... Current FPS: 10.98\nProcessed frame 368/900... Current FPS: 10.99\nProcessed frame 372/900... Current FPS: 10.99\nProcessed frame 376/900... Current FPS: 11.00\nProcessed frame 380/900... Current FPS: 11.01\nProcessed frame 384/900... Current FPS: 11.03\nProcessed frame 388/900... Current FPS: 11.05\nProcessed frame 392/900... Current FPS: 11.06\nProcessed frame 396/900... Current FPS: 11.08\nProcessed frame 400/900... Current FPS: 11.09\nProcessed frame 404/900... Current FPS: 11.10\nProcessed frame 408/900... Current FPS: 11.12\nProcessed frame 412/900... Current FPS: 11.12\nProcessed frame 416/900... Current FPS: 11.13\nProcessed frame 420/900... Current FPS: 11.14\nProcessed frame 424/900... Current FPS: 11.16\nProcessed frame 428/900... Current FPS: 11.18\nProcessed frame 432/900... Current FPS: 11.19\nProcessed frame 436/900... Current FPS: 11.20\nProcessed frame 440/900... Current FPS: 11.21\nProcessed frame 444/900... Current FPS: 11.21\nProcessed frame 448/900... Current FPS: 11.22\nProcessed frame 452/900... Current FPS: 11.23\nProcessed frame 456/900... Current FPS: 11.24\nProcessed frame 460/900... Current FPS: 11.26\nProcessed frame 464/900... Current FPS: 11.27\nProcessed frame 468/900... Current FPS: 11.28\nProcessed frame 472/900... Current FPS: 11.30\nProcessed frame 476/900... Current FPS: 11.31\nProcessed frame 480/900... Current FPS: 11.32\nProcessed frame 484/900... Current FPS: 11.33\nProcessed frame 488/900... Current FPS: 11.34\nProcessed frame 492/900... Current FPS: 11.36\nProcessed frame 496/900... Current FPS: 11.37\nProcessed frame 500/900... Current FPS: 11.38\nProcessed frame 504/900... Current FPS: 11.39\nProcessed frame 508/900... Current FPS: 11.40\nProcessed frame 512/900... Current FPS: 11.41\nProcessed frame 516/900... Current FPS: 11.42\nProcessed frame 520/900... Current FPS: 11.43\nProcessed frame 524/900... Current FPS: 11.44\nProcessed frame 528/900... Current FPS: 11.45\nProcessed frame 532/900... Current FPS: 11.46\nProcessed frame 536/900... Current FPS: 11.48\nProcessed frame 540/900... Current FPS: 11.49\nProcessed frame 544/900... Current FPS: 11.49\nProcessed frame 548/900... Current FPS: 11.50\nProcessed frame 552/900... Current FPS: 11.51\nProcessed frame 556/900... Current FPS: 11.52\nProcessed frame 560/900... Current FPS: 11.53\nProcessed frame 564/900... Current FPS: 11.53\nProcessed frame 568/900... Current FPS: 11.53\nProcessed frame 572/900... Current FPS: 11.53\nProcessed frame 576/900... Current FPS: 11.54\nProcessed frame 580/900... Current FPS: 11.55\nProcessed frame 584/900... Current FPS: 11.55\nProcessed frame 588/900... Current FPS: 11.56\nProcessed frame 592/900... Current FPS: 11.57\nProcessed frame 596/900... Current FPS: 11.57\nProcessed frame 600/900... Current FPS: 11.58\nProcessed frame 604/900... Current FPS: 11.59\nProcessed frame 608/900... Current FPS: 11.60\nProcessed frame 612/900... Current FPS: 11.58\nProcessed frame 616/900... Current FPS: 11.59\nProcessed frame 620/900... Current FPS: 11.59\nProcessed frame 624/900... Current FPS: 11.58\nProcessed frame 628/900... Current FPS: 11.58\nProcessed frame 632/900... Current FPS: 11.59\nProcessed frame 636/900... Current FPS: 11.60\nProcessed frame 640/900... Current FPS: 11.60\nProcessed frame 644/900... Current FPS: 11.61\nProcessed frame 648/900... Current FPS: 11.62\nProcessed frame 652/900... Current FPS: 11.62\nProcessed frame 656/900... Current FPS: 11.63\nProcessed frame 660/900... Current FPS: 11.64\nProcessed frame 664/900... Current FPS: 11.65\nProcessed frame 668/900... Current FPS: 11.66\nProcessed frame 672/900... Current FPS: 11.67\nProcessed frame 676/900... Current FPS: 11.67\nProcessed frame 680/900... Current FPS: 11.68\nProcessed frame 684/900... Current FPS: 11.69\nProcessed frame 688/900... Current FPS: 11.70\nProcessed frame 692/900... Current FPS: 11.71\nProcessed frame 696/900... Current FPS: 11.72\nProcessed frame 700/900... Current FPS: 11.73\nProcessed frame 704/900... Current FPS: 11.73\nProcessed frame 708/900... Current FPS: 11.74\nProcessed frame 712/900... Current FPS: 11.75\nProcessed frame 716/900... Current FPS: 11.75\nProcessed frame 720/900... Current FPS: 11.76\nProcessed frame 724/900... Current FPS: 11.77\nProcessed frame 728/900... Current FPS: 11.77\nProcessed frame 732/900... Current FPS: 11.78\nProcessed frame 736/900... Current FPS: 11.79\nProcessed frame 740/900... Current FPS: 11.80\nProcessed frame 744/900... Current FPS: 11.81\nProcessed frame 748/900... Current FPS: 11.82\nProcessed frame 752/900... Current FPS: 11.82\nProcessed frame 756/900... Current FPS: 11.83\nProcessed frame 760/900... Current FPS: 11.83\nProcessed frame 764/900... Current FPS: 11.84\nProcessed frame 768/900... Current FPS: 11.85\nProcessed frame 772/900... Current FPS: 11.85\nProcessed frame 776/900... Current FPS: 11.86\nProcessed frame 780/900... Current FPS: 11.87\nProcessed frame 784/900... Current FPS: 11.88\nProcessed frame 788/900... Current FPS: 11.89\nProcessed frame 792/900... Current FPS: 11.89\nProcessed frame 796/900... Current FPS: 11.90\nProcessed frame 800/900... Current FPS: 11.91\nProcessed frame 804/900... Current FPS: 11.92\nProcessed frame 808/900... Current FPS: 11.93\nProcessed frame 812/900... Current FPS: 11.94\nProcessed frame 816/900... Current FPS: 11.95\nProcessed frame 820/900... Current FPS: 11.96\nProcessed frame 824/900... Current FPS: 11.97\nProcessed frame 828/900... Current FPS: 11.98\nProcessed frame 832/900... Current FPS: 11.99\nProcessed frame 836/900... Current FPS: 12.00\nProcessed frame 840/900... Current FPS: 12.01\nProcessed frame 844/900... Current FPS: 12.02\nProcessed frame 848/900... Current FPS: 12.03\nProcessed frame 852/900... Current FPS: 12.04\nProcessed frame 856/900... Current FPS: 12.05\nProcessed frame 860/900... Current FPS: 12.06\nProcessed frame 864/900... Current FPS: 12.07\nProcessed frame 868/900... Current FPS: 12.08\nProcessed frame 872/900... Current FPS: 12.09\nProcessed frame 876/900... Current FPS: 12.10\nProcessed frame 880/900... Current FPS: 12.10\nProcessed frame 884/900... Current FPS: 12.11\nProcessed frame 888/900... Current FPS: 12.12\nProcessed frame 892/900... Current FPS: 12.13\nProcessed frame 896/900... Current FPS: 12.14\nProcessed frame 900/900... Current FPS: 12.14\n\n--- Analysis Complete ---\nOutput video saved to: /kaggle/working/highway_intelligence_final.mp4\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# --- IMPORTS ---\nimport torch\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon, LineString\nimport time\nfrom collections import defaultdict, deque\nimport pandas as pd\n\n# --- CONFIGURATION ---\nprint(\"\\n--- Initializing Configuration ---\")\n\n# --- CORE PARAMETERS ---\nMODEL_PATH = '/kaggle/working/runs/train/yolov8l_dropout_run/weights/best.pt'\nVIDEO_PATH = '/kaggle/input/videohai/traffic.mp4'\nOUTPUT_VIDEO_PATH = '/kaggle/working/traffic_platform_final.mp4'\nCSV_LOG_PATH = '/kaggle/working/traffic_platform_events_final.csv'\n\n# --- DETECTION & TRACKING ---\nTARGET_CLASSES = [0, 1, 2, 3]\nCLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle']\nCONF_THRESHOLD = 0.3\nIOU_THRESHOLD = 0.5\nDEEPSORT_CONFIG = {'max_age': 50, 'n_init': 5, 'nms_max_overlap': 1.0, 'nn_budget': 100}\nBATCH_SIZE = 4\n\n# --- ANALYSIS & VISUALIZATION ---\nLANE_LINE_DEFINITIONS = [\n    [(100, 720), (500, 450)], [(550, 720), (650, 450)],\n    [(800, 720), (850, 450)], [(1200, 720), (1100, 450)]\n]\nCOUNTING_LINE_Y_POSITION = 400\nDENSITY_THRESHOLDS = {'normal': 0.15, 'medium': 0.30}\nSPEED_CALIBRATION_FACTOR = 3.5 \nSPEED_LIMIT_KMPH = 100\nSPEED_UNIT = 'km/h'\nSTOPPED_VEHICLE_SPEED_THRESHOLD_KMPH = 5\nSTOPPED_VEHICLE_DURATION_SECONDS = 5\nDASHBOARD_TRANSPARENCY = 0.7\n\n# --- ADVANCED FEATURE CONFIGURATION ---\nLANE_CHANGE_COOLDOWN_FRAMES = 15\nSUDDEN_BRAKING_THRESHOLD_FACTOR = 0.5\nHEATMAP_DECAY_RATE = 0.98\n\n# --- INITIALIZATION ---\nprint(\"--- Initializing Systems ---\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = YOLO(MODEL_PATH)\nmodel.to(device)\ntracker = DeepSort(max_age=DEEPSORT_CONFIG['max_age'], n_init=DEEPSORT_CONFIG['n_init'], nms_max_overlap=DEEPSORT_CONFIG['nms_max_overlap'], nn_budget=DEEPSORT_CONFIG['nn_budget'])\ncap = cv2.VideoCapture(VIDEO_PATH)\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nout = cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n\n# --- DATA STRUCTURES ---\nlane_polygons = [Polygon(LANE_LINE_DEFINITIONS[i] + LANE_LINE_DEFINITIONS[i+1][::-1]) for i in range(len(LANE_LINE_DEFINITIONS) - 1)]\nlane_areas_pixels = [poly.area for poly in lane_polygons]\ntrack_history = defaultdict(lambda: deque(maxlen=int(fps)))\nstopped_timers, event_log = defaultdict(int), []\ncumulative_counts_by_lane, counted_ids_by_lane = defaultdict(int), defaultdict(set)\ncounting_line = LineString([(0, COUNTING_LINE_Y_POSITION), (w, COUNTING_LINE_Y_POSITION)])\nlane_speed_history = defaultdict(lambda: deque(maxlen=int(fps * 2)))\ntrack_speed_history = defaultdict(lambda: deque(maxlen=5))\ntrack_lane_history = defaultdict(int)\nlane_change_timers = defaultdict(int)\nheatmap_layer = np.zeros((h, w), dtype=np.float32)\n\n# --- MAIN PROCESSING LOOP ---\nprint(f\"--- Starting Advanced Analysis ---\")\nframes_batch, frame_num, start_time = [], 0, time.time()\n\ntry:\n    while cap.isOpened():\n        ret, frame = cap.read();\n        if not ret: break\n        frames_batch.append(frame); frame_num += 1\n\n        if len(frames_batch) == BATCH_SIZE or frame_num == int(cap.get(cv2.CAP_PROP_FRAME_COUNT)):\n            results_batch = model.predict(frames_batch, classes=TARGET_CLASSES, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD, verbose=False)\n\n            for i, results in enumerate(results_batch):\n                current_frame = frames_batch[i]\n                \n                detections = []\n                for box in results.boxes:\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    detections.append(([int(x1), int(y1), int(x2 - x1), int(y2 - y1)], box.conf[0].item(), CLASS_NAMES[int(box.cls[0].item())]))\n                \n                tracks = tracker.update_tracks(detections, frame=current_frame)\n                current_lane_counts, lane_vehicle_areas = defaultdict(int), defaultdict(float)\n                heatmap_layer = cv2.addWeighted(heatmap_layer, HEATMAP_DECAY_RATE, np.zeros_like(heatmap_layer), 1 - HEATMAP_DECAY_RATE, 0)\n                all_moving_speeds = []\n\n                for track in tracks:\n                    # ... Main track processing ... (This part remains the same)\n                    if not track.is_confirmed() or track.time_since_update > 1: continue\n                    track_id, bbox, class_name = track.track_id, track.to_ltrb(), track.get_det_class()\n                    bottom_center = Point((bbox[0] + bbox[2]) / 2, bbox[3])\n                    track_history[track_id].append(bottom_center)\n                    cv2.circle(heatmap_layer, (int(bottom_center.x), int(bottom_center.y)), 10, (255), -1)\n                    # ... All the speed, behavior, and lane logic ...\n                    speed_kmph, assigned_lane = -1, next((i + 1 for i, poly in enumerate(lane_polygons) if poly.contains(bottom_center)), None)\n                    if len(track_history[track_id]) > int(fps / 2):\n                        prev_pos, time_diff = track_history[track_id][0], len(track_history[track_id]) / fps\n                        pixel_dist, perspective_scale = bottom_center.distance(prev_pos), 1 - (bottom_center.y / h)\n                        real_world_dist = pixel_dist * perspective_scale * (REAL_WORLD_LANE_LENGTH_METERS / h)\n                        speed_kmph = (real_world_dist / time_diff * 3.6 if time_diff > 0 else 0) * SPEED_CALIBRATION_FACTOR\n                    if assigned_lane:\n                        current_lane_counts[assigned_lane] += 1\n                        lane_vehicle_areas[assigned_lane] += (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n                        if speed_kmph > STOPPED_VEHICLE_SPEED_THRESHOLD_KMPH:\n                            lane_speed_history[assigned_lane].append(speed_kmph)\n                            all_moving_speeds.append(speed_kmph)\n                    is_stopped, is_speeding, is_braking, is_changing_lanes = False, False, False, False\n                    is_speeding = speed_kmph > SPEED_LIMIT_KMPH\n                    if speed_kmph != -1 and speed_kmph < STOPPED_VEHICLE_SPEED_THRESHOLD_KMPH:\n                        stopped_timers[track_id] += 1\n                        if (stopped_timers[track_id] / fps) > STOPPED_VEHICLE_DURATION_SECONDS: is_stopped = True\n                    else: stopped_timers[track_id] = 0\n                    if track_speed_history[track_id] and speed_kmph != -1:\n                        if speed_kmph < track_speed_history[track_id][-1] * SUDDEN_BRAKING_THRESHOLD_FACTOR: is_braking = True\n                    track_speed_history[track_id].append(speed_kmph)\n                    if assigned_lane and track_lane_history[track_id] != 0 and assigned_lane != track_lane_history[track_id]:\n                        is_changing_lanes = True; lane_change_timers[track_id] = LANE_CHANGE_COOLDOWN_FRAMES\n                    if lane_change_timers[track_id] > 0:\n                        is_changing_lanes = True; lane_change_timers[track_id] -= 1\n                    track_lane_history[track_id] = assigned_lane\n                    if is_speeding: label, color = f\"{class_name} {track_id} [SPEEDING]\", (255, 0, 255)\n                    elif is_stopped: label, color = f\"{class_name} {track_id} [STOPPED]\", (0, 0, 255)\n                    elif is_braking: label, color = f\"{class_name} {track_id} [BRAKING]\", (0, 165, 255)\n                    elif is_changing_lanes: label, color = f\"{class_name} {track_id} [LANE CHANGE]\", (255, 255, 0)\n                    else:\n                        label = f\"{class_name} {track_id} {int(speed_kmph)} {SPEED_UNIT}\" if speed_kmph > 0 else f\"{class_name} {track_id}\"\n                        color = (0, 255, 0)\n                    cv2.rectangle(current_frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n                    cv2.putText(current_frame, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n                \n                # --- VISUALIZATION ---\n                # --- THIS IS THE FIX ---\n                # Step 1: Normalize the heatmap to a float array in the range [0, 255]\n                heatmap_normalized_float = cv2.normalize(heatmap_layer, None, 0, 255, cv2.NORM_MINMAX)\n                # Step 2: Convert the float array to an 8-bit unsigned integer array\n                heatmap_uint8 = heatmap_normalized_float.astype(np.uint8)\n                # Step 3: Apply the colormap\n                heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n\n                final_frame = cv2.addWeighted(current_frame, 0.6, heatmap_colored, 0.4, 0)\n                dashboard_panel = np.zeros((200, w, 3), dtype=np.uint8)\n                y_offset = 30\n                avg_scene_speed = int(np.mean(all_moving_speeds)) if all_moving_speeds else 0\n                cv2.putText(dashboard_panel, f\"Scene Analytics: {sum(current_lane_counts.values())} Vehicles Visible | Avg Speed: {avg_scene_speed} {SPEED_UNIT}\", (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n                y_offset += 40\n\n                for lane_num, poly in enumerate(lane_polygons, 1):\n                    # ... (Dashboard text logic remains the same) ...\n                    avg_lane_speed = int(np.mean(lane_speed_history[lane_num])) if lane_speed_history[lane_num] else 0\n                    trend = \"→\"\n                    if len(lane_speed_history[lane_num]) > 10:\n                        recent_avg = np.mean(list(lane_speed_history[lane_num])[-10:])\n                        older_avg = np.mean(list(lane_speed_history[lane_num])[:-10])\n                        if recent_avg > older_avg * 1.1: trend = \"↑\"\n                        elif recent_avg < older_avg * 0.9: trend = \"↓\"\n                    density_ratio = lane_vehicle_areas[lane_num] / lane_areas_pixels[lane_num - 1] if lane_areas_pixels[lane_num - 1] > 0 else 0\n                    density_level = \"High\" if density_ratio > DENSITY_THRESHOLDS['medium'] else \"Medium\" if density_ratio > DENSITY_THRESHOLDS['normal'] else \"Normal\"\n                    info_text = f\"Lane {lane_num}: {density_level} ({density_ratio:.1%}) | Avg Speed: {avg_lane_speed} {SPEED_UNIT} {trend} | Live: {current_lane_counts[lane_num]}\"\n                    cv2.putText(dashboard_panel, info_text, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n                    y_offset += 30\n\n                roi = final_frame[0:200, 0:w]\n                blended_roi = cv2.addWeighted(roi, 1.0, dashboard_panel, DASHBOARD_TRANSPARENCY, 0)\n                final_frame[0:200, 0:w] = blended_roi\n                out.write(final_frame)\n\n            frames_batch = [];\n            print(f\"Processed frame {frame_num}/{int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}...\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    cap.release(); out.release(); print(\"\\n--- Analysis Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T17:16:12.895184Z","iopub.execute_input":"2025-06-17T17:16:12.895482Z","iopub.status.idle":"2025-06-17T17:17:29.514315Z","shell.execute_reply.started":"2025-06-17T17:16:12.895460Z","shell.execute_reply":"2025-06-17T17:17:29.513653Z"}},"outputs":[{"name":"stdout","text":"\n--- Initializing Configuration ---\n--- Initializing Systems ---\n--- Starting Advanced Analysis ---\nProcessed frame 4/900...\nProcessed frame 8/900...\nProcessed frame 12/900...\nProcessed frame 16/900...\nProcessed frame 20/900...\nProcessed frame 24/900...\nProcessed frame 28/900...\nProcessed frame 32/900...\nProcessed frame 36/900...\nProcessed frame 40/900...\nProcessed frame 44/900...\nProcessed frame 48/900...\nProcessed frame 52/900...\nProcessed frame 56/900...\nProcessed frame 60/900...\nProcessed frame 64/900...\nProcessed frame 68/900...\nProcessed frame 72/900...\nProcessed frame 76/900...\nProcessed frame 80/900...\nProcessed frame 84/900...\nProcessed frame 88/900...\nProcessed frame 92/900...\nProcessed frame 96/900...\nProcessed frame 100/900...\nProcessed frame 104/900...\nProcessed frame 108/900...\nProcessed frame 112/900...\nProcessed frame 116/900...\nProcessed frame 120/900...\nProcessed frame 124/900...\nProcessed frame 128/900...\nProcessed frame 132/900...\nProcessed frame 136/900...\nProcessed frame 140/900...\nProcessed frame 144/900...\nProcessed frame 148/900...\nProcessed frame 152/900...\nProcessed frame 156/900...\nProcessed frame 160/900...\nProcessed frame 164/900...\nProcessed frame 168/900...\nProcessed frame 172/900...\nProcessed frame 176/900...\nProcessed frame 180/900...\nProcessed frame 184/900...\nProcessed frame 188/900...\nProcessed frame 192/900...\nProcessed frame 196/900...\nProcessed frame 200/900...\nProcessed frame 204/900...\nProcessed frame 208/900...\nProcessed frame 212/900...\nProcessed frame 216/900...\nProcessed frame 220/900...\nProcessed frame 224/900...\nProcessed frame 228/900...\nProcessed frame 232/900...\nProcessed frame 236/900...\nProcessed frame 240/900...\nProcessed frame 244/900...\nProcessed frame 248/900...\nProcessed frame 252/900...\nProcessed frame 256/900...\nProcessed frame 260/900...\nProcessed frame 264/900...\nProcessed frame 268/900...\nProcessed frame 272/900...\nProcessed frame 276/900...\nProcessed frame 280/900...\nProcessed frame 284/900...\nProcessed frame 288/900...\nProcessed frame 292/900...\nProcessed frame 296/900...\nProcessed frame 300/900...\nProcessed frame 304/900...\nProcessed frame 308/900...\nProcessed frame 312/900...\nProcessed frame 316/900...\nProcessed frame 320/900...\nProcessed frame 324/900...\nProcessed frame 328/900...\nProcessed frame 332/900...\nProcessed frame 336/900...\nProcessed frame 340/900...\nProcessed frame 344/900...\nProcessed frame 348/900...\nProcessed frame 352/900...\nProcessed frame 356/900...\nProcessed frame 360/900...\nProcessed frame 364/900...\nProcessed frame 368/900...\nProcessed frame 372/900...\nProcessed frame 376/900...\nProcessed frame 380/900...\nProcessed frame 384/900...\nProcessed frame 388/900...\nProcessed frame 392/900...\nProcessed frame 396/900...\nProcessed frame 400/900...\nProcessed frame 404/900...\nProcessed frame 408/900...\nProcessed frame 412/900...\nProcessed frame 416/900...\nProcessed frame 420/900...\nProcessed frame 424/900...\nProcessed frame 428/900...\nProcessed frame 432/900...\nProcessed frame 436/900...\nProcessed frame 440/900...\nProcessed frame 444/900...\nProcessed frame 448/900...\nProcessed frame 452/900...\nProcessed frame 456/900...\nProcessed frame 460/900...\nProcessed frame 464/900...\nProcessed frame 468/900...\nProcessed frame 472/900...\nProcessed frame 476/900...\nProcessed frame 480/900...\nProcessed frame 484/900...\nProcessed frame 488/900...\nProcessed frame 492/900...\nProcessed frame 496/900...\nProcessed frame 500/900...\nProcessed frame 504/900...\nProcessed frame 508/900...\nProcessed frame 512/900...\nProcessed frame 516/900...\nProcessed frame 520/900...\nProcessed frame 524/900...\nProcessed frame 528/900...\nProcessed frame 532/900...\nProcessed frame 536/900...\nProcessed frame 540/900...\nProcessed frame 544/900...\nProcessed frame 548/900...\nProcessed frame 552/900...\nProcessed frame 556/900...\nProcessed frame 560/900...\nProcessed frame 564/900...\nProcessed frame 568/900...\nProcessed frame 572/900...\nProcessed frame 576/900...\nProcessed frame 580/900...\nProcessed frame 584/900...\nProcessed frame 588/900...\nProcessed frame 592/900...\nProcessed frame 596/900...\nProcessed frame 600/900...\nProcessed frame 604/900...\nProcessed frame 608/900...\nProcessed frame 612/900...\nProcessed frame 616/900...\nProcessed frame 620/900...\nProcessed frame 624/900...\nProcessed frame 628/900...\nProcessed frame 632/900...\nProcessed frame 636/900...\nProcessed frame 640/900...\nProcessed frame 644/900...\nProcessed frame 648/900...\nProcessed frame 652/900...\nProcessed frame 656/900...\nProcessed frame 660/900...\nProcessed frame 664/900...\nProcessed frame 668/900...\nProcessed frame 672/900...\nProcessed frame 676/900...\nProcessed frame 680/900...\nProcessed frame 684/900...\nProcessed frame 688/900...\nProcessed frame 692/900...\nProcessed frame 696/900...\nProcessed frame 700/900...\nProcessed frame 704/900...\nProcessed frame 708/900...\nProcessed frame 712/900...\nProcessed frame 716/900...\nProcessed frame 720/900...\nProcessed frame 724/900...\nProcessed frame 728/900...\nProcessed frame 732/900...\nProcessed frame 736/900...\nProcessed frame 740/900...\nProcessed frame 744/900...\nProcessed frame 748/900...\nProcessed frame 752/900...\nProcessed frame 756/900...\nProcessed frame 760/900...\nProcessed frame 764/900...\nProcessed frame 768/900...\nProcessed frame 772/900...\nProcessed frame 776/900...\nProcessed frame 780/900...\nProcessed frame 784/900...\nProcessed frame 788/900...\nProcessed frame 792/900...\nProcessed frame 796/900...\nProcessed frame 800/900...\nProcessed frame 804/900...\nProcessed frame 808/900...\nProcessed frame 812/900...\nProcessed frame 816/900...\nProcessed frame 820/900...\nProcessed frame 824/900...\nProcessed frame 828/900...\nProcessed frame 832/900...\nProcessed frame 836/900...\nProcessed frame 840/900...\nProcessed frame 844/900...\nProcessed frame 848/900...\nProcessed frame 852/900...\nProcessed frame 856/900...\nProcessed frame 860/900...\nProcessed frame 864/900...\nProcessed frame 868/900...\nProcessed frame 872/900...\nProcessed frame 876/900...\nProcessed frame 880/900...\nProcessed frame 884/900...\nProcessed frame 888/900...\nProcessed frame 892/900...\nProcessed frame 896/900...\nProcessed frame 900/900...\n\n--- Analysis Complete ---\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# --- IMPORTS ---\nimport torch\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon, LineString\nimport time\nfrom collections import defaultdict, deque\nimport pandas as pd\n\n# --- CONFIGURATION ---\nprint(\"\\n--- Initializing Configuration ---\")\n\n# --- CORE PARAMETERS ---\nMODEL_PATH = '/kaggle/working/runs/train/yolov8l_dropout_run/weights/best.pt'\nVIDEO_PATH = '/kaggle/input/videohai/traffic.mp4'\nOUTPUT_VIDEO_PATH = '/kaggle/working/traffic_LoS_analysis_final.mp4'\nCSV_LOG_PATH = '/kaggle/working/traffic_LoS_events_final.csv'\n\n# --- DETECTION & TRACKING ---\nTARGET_CLASSES = [0, 1, 2, 3]\nCLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle']\nCONF_THRESHOLD = 0.3\nIOU_THRESHOLD = 0.5\nDEEPSORT_CONFIG = {'max_age': 50, 'n_init': 5, 'nms_max_overlap': 1.0, 'nn_budget': 100}\nBATCH_SIZE = 4\n\n# --- ADVANCED TRAFFIC ANALYSIS CONFIGURATION ---\nLANE_LINE_DEFINITIONS = [\n    [(100, 720), (500, 450)], [(550, 720), (650, 450)],\n    [(800, 720), (850, 450)], [(1200, 720), (1100, 450)]\n]\nCOUNTING_LINE_Y_POSITION = 400\nDASHBOARD_TRANSPARENCY = 0.7\nSPEED_CALIBRATION_FACTOR = 3.5 \nREAL_WORLD_LANE_LENGTH_METERS = 30 \nSPEED_UNIT = 'km/h'\nLOS_THRESHOLDS = {'A': 0.10, 'B': 0.18, 'C': 0.26, 'D': 0.35, 'E': 0.45}\nLOS_COLORS = {'A': (0, 255, 0), 'B': (100, 255, 0), 'C': (200, 255, 0), 'D': (255, 200, 0), 'E': (255, 100, 0), 'F': (255, 0, 0)}\nFLOW_RATE_WINDOW_SECONDS = 60\nQUEUE_DETECTION_SPEED_KMPH = 10\nQUEUE_MIN_VEHICLES = 3\n\n# --- INITIALIZATION ---\nprint(\"--- Initializing Systems ---\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = YOLO(MODEL_PATH)\nmodel.to(device)\ntracker = DeepSort(max_age=DEEPSORT_CONFIG['max_age'], n_init=DEEPSORT_CONFIG['n_init'], nms_max_overlap=DEEPSORT_CONFIG['nms_max_overlap'], nn_budget=DEEPSORT_CONFIG['nn_budget'])\ncap = cv2.VideoCapture(VIDEO_PATH)\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nout = cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n\n# --- DATA STRUCTURES ---\nlane_polygons = [Polygon(LANE_LINE_DEFINITIONS[i] + LANE_LINE_DEFINITIONS[i+1][::-1]) for i in range(len(LANE_LINE_DEFINITIONS) - 1)]\nlane_areas_pixels = [poly.area for poly in lane_polygons]\ntrack_history = defaultdict(lambda: deque(maxlen=int(fps)))\nevent_log = []\ncounting_line = LineString([(0, COUNTING_LINE_Y_POSITION), (w, COUNTING_LINE_Y_POSITION)])\nlane_speed_history = defaultdict(lambda: deque(maxlen=int(fps * 2)))\nflow_rate_timestamps = defaultdict(lambda: deque())\ntrack_statuses = {}\n\n# --- MAIN PROCESSING LOOP ---\nprint(f\"--- Starting Level of Service (LoS) Analysis ---\")\nframes_batch, frame_num, start_time = [], 0, time.time()\n\ntry:\n    while cap.isOpened():\n        ret, frame = cap.read();\n        if not ret: break\n        frames_batch.append(frame); frame_num += 1\n\n        if len(frames_batch) == BATCH_SIZE or frame_num == int(cap.get(cv2.CAP_PROP_FRAME_COUNT)):\n            results_batch = model.predict(frames_batch, classes=TARGET_CLASSES, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD, verbose=False)\n\n            for i, results in enumerate(results_batch):\n                current_frame = frames_batch[i]\n                current_frame_time = (frame_num - len(frames_batch) + i) / fps\n                \n                # --- THIS IS THE FIX ---\n                # We must manually build the detections list in the correct format for DeepSORT\n                detections = []\n                for box in results.boxes:\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    w_box, h_box = x2 - x1, y2 - y1\n                    conf = box.conf[0].item()\n                    cls = CLASS_NAMES[int(box.cls[0].item())]\n                    detections.append(([int(x1), int(y1), int(w_box), int(h_box)], conf, cls))\n                \n                # Now pass the correctly formatted list to the tracker\n                tracks = tracker.update_tracks(detections, frame=current_frame)\n                \n                current_lane_counts, lane_vehicle_areas = defaultdict(int), defaultdict(float)\n                track_statuses.clear()\n\n                for track in tracks:\n                    # ... (The rest of the logic remains the same) ...\n                    if not track.is_confirmed() or track.time_since_update > 1: continue\n                    track_id, bbox, class_name = track.track_id, track.to_ltrb(), track.get_det_class()\n                    bottom_center = Point((bbox[0] + bbox[2]) / 2, bbox[3])\n                    track_history[track_id].append(bottom_center)\n                    speed_kmph, assigned_lane = -1, next((i + 1 for i, poly in enumerate(lane_polygons) if poly.contains(bottom_center)), None)\n                    if len(track_history[track_id]) > int(fps / 2):\n                        prev_pos, time_diff = track_history[track_id][0], len(track_history[track_id]) / fps\n                        pixel_dist, perspective_scale = bottom_center.distance(prev_pos), 1 - (bottom_center.y / h)\n                        real_world_dist = pixel_dist * perspective_scale * (REAL_WORLD_LANE_LENGTH_METERS / h)\n                        speed_kmph = (real_world_dist / time_diff * 3.6 if time_diff > 0 else 0) * SPEED_CALIBRATION_FACTOR\n                    track_statuses[track_id] = {'speed': speed_kmph, 'bbox': bbox, 'lane': assigned_lane}\n                    if assigned_lane:\n                        current_lane_counts[assigned_lane] += 1\n                        lane_vehicle_areas[assigned_lane] += (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n                        if speed_kmph > 0: lane_speed_history[assigned_lane].append(speed_kmph)\n                    if len(track_history[track_id]) > 1:\n                        if LineString([track_history[track_id][-2], track_history[track_id][-1]]).intersects(counting_line):\n                            flow_rate_timestamps[assigned_lane].append(current_frame_time)\n                    label = f\"{class_name} {track_id} {int(speed_kmph)} {SPEED_UNIT}\" if speed_kmph > 0 else f\"{class_name} {track_id}\"\n                    cv2.rectangle(current_frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n                    cv2.putText(current_frame, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n                dashboard_panel = np.zeros((220, w, 3), dtype=np.uint8)\n                y_offset = 30\n                for lane_num in range(1, len(lane_polygons) + 1):\n                    queued_vehicles = [t for t_id, t in track_statuses.items() if t['lane'] == lane_num and t['speed'] != -1 and t['speed'] < QUEUE_DETECTION_SPEED_KMPH]\n                    if len(queued_vehicles) >= QUEUE_MIN_VEHICLES:\n                        min_y, max_y = min(v['bbox'][1] for v in queued_vehicles), max(v['bbox'][3] for v in queued_vehicles)\n                        queue_pixel_length, avg_y = max_y - min_y, (min_y + max_y) / 2\n                        perspective_scale = 1 - (avg_y / h)\n                        queue_meters = queue_pixel_length * perspective_scale * (REAL_WORLD_LANE_LENGTH_METERS / h)\n                        poly_pts = np.array(lane_polygons[lane_num-1].exterior.coords, np.int32)\n                        overlay = current_frame.copy()\n                        cv2.fillPoly(overlay, [poly_pts], (0, 0, 150))\n                        current_frame = cv2.addWeighted(overlay, 0.4, current_frame, 0.6, 0)\n                        queue_x_pos = int(lane_polygons[lane_num-1].centroid.x)\n                        cv2.line(current_frame, (queue_x_pos + 20, int(min_y)), (queue_x_pos + 20, int(max_y)), (0, 255, 255), 3)\n                        cv2.putText(current_frame, f\"{queue_meters:.1f}m Queue\", (queue_x_pos + 25, int(avg_y)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n\n                for lane_num, poly in enumerate(lane_polygons, 1):\n                    density_ratio = lane_vehicle_areas[lane_num] / lane_areas_pixels[lane_num - 1] if lane_areas_pixels[lane_num - 1] > 0 else 0\n                    los = 'F';\n                    for grade, threshold in sorted(LOS_THRESHOLDS.items()):\n                        if density_ratio <= threshold: los = grade; break\n                    while flow_rate_timestamps[lane_num] and flow_rate_timestamps[lane_num][0] < current_frame_time - FLOW_RATE_WINDOW_SECONDS:\n                        flow_rate_timestamps[lane_num].popleft()\n                    flow_rate_vph = len(flow_rate_timestamps[lane_num]) * (3600 / FLOW_RATE_WINDOW_SECONDS)\n                    avg_lane_speed = int(np.mean(lane_speed_history[lane_num])) if lane_speed_history[lane_num] else 0\n                    trend = \"→\"\n                    if len(lane_speed_history[lane_num]) > fps:\n                        recent_avg, older_avg = np.mean(list(lane_speed_history[lane_num])[-int(fps/2):]), np.mean(list(lane_speed_history[lane_num])[:-int(fps/2)])\n                        if recent_avg > older_avg * 1.1: trend = \"↑\"\n                        elif recent_avg < older_avg * 0.9: trend = \"↓\"\n                    info_text = f\"Lane {lane_num} | LoS: {los} | Flow: {int(flow_rate_vph)} VPH | Avg Speed: {avg_lane_speed} {trend} | Live: {current_lane_counts[lane_num]}\"\n                    cv2.putText(dashboard_panel, info_text, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, LOS_COLORS[los], 2)\n                    y_offset += 30\n\n                roi = current_frame[0:220, 0:w]\n                blended_roi = cv2.addWeighted(roi, 1.0, dashboard_panel, DASHBOARD_TRANSPARENCY, 0)\n                current_frame[0:220, 0:w] = blended_roi\n                out.write(current_frame)\n\n            frames_batch = []; print(f\"Processed frame {frame_num}/{int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}...\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    cap.release(); out.release(); print(\"\\n--- Analysis Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T17:23:39.070809Z","iopub.execute_input":"2025-06-17T17:23:39.071120Z","iopub.status.idle":"2025-06-17T17:24:52.441821Z","shell.execute_reply.started":"2025-06-17T17:23:39.071099Z","shell.execute_reply":"2025-06-17T17:24:52.441165Z"}},"outputs":[{"name":"stdout","text":"\n--- Initializing Configuration ---\n--- Initializing Systems ---\n--- Starting Level of Service (LoS) Analysis ---\nProcessed frame 4/900...\nProcessed frame 8/900...\nProcessed frame 12/900...\nProcessed frame 16/900...\nProcessed frame 20/900...\nProcessed frame 24/900...\nProcessed frame 28/900...\nProcessed frame 32/900...\nProcessed frame 36/900...\nProcessed frame 40/900...\nProcessed frame 44/900...\nProcessed frame 48/900...\nProcessed frame 52/900...\nProcessed frame 56/900...\nProcessed frame 60/900...\nProcessed frame 64/900...\nProcessed frame 68/900...\nProcessed frame 72/900...\nProcessed frame 76/900...\nProcessed frame 80/900...\nProcessed frame 84/900...\nProcessed frame 88/900...\nProcessed frame 92/900...\nProcessed frame 96/900...\nProcessed frame 100/900...\nProcessed frame 104/900...\nProcessed frame 108/900...\nProcessed frame 112/900...\nProcessed frame 116/900...\nProcessed frame 120/900...\nProcessed frame 124/900...\nProcessed frame 128/900...\nProcessed frame 132/900...\nProcessed frame 136/900...\nProcessed frame 140/900...\nProcessed frame 144/900...\nProcessed frame 148/900...\nProcessed frame 152/900...\nProcessed frame 156/900...\nProcessed frame 160/900...\nProcessed frame 164/900...\nProcessed frame 168/900...\nProcessed frame 172/900...\nProcessed frame 176/900...\nProcessed frame 180/900...\nProcessed frame 184/900...\nProcessed frame 188/900...\nProcessed frame 192/900...\nProcessed frame 196/900...\nProcessed frame 200/900...\nProcessed frame 204/900...\nProcessed frame 208/900...\nProcessed frame 212/900...\nProcessed frame 216/900...\nProcessed frame 220/900...\nProcessed frame 224/900...\nProcessed frame 228/900...\nProcessed frame 232/900...\nProcessed frame 236/900...\nProcessed frame 240/900...\nProcessed frame 244/900...\nProcessed frame 248/900...\nProcessed frame 252/900...\nProcessed frame 256/900...\nProcessed frame 260/900...\nProcessed frame 264/900...\nProcessed frame 268/900...\nProcessed frame 272/900...\nProcessed frame 276/900...\nProcessed frame 280/900...\nProcessed frame 284/900...\nProcessed frame 288/900...\nProcessed frame 292/900...\nProcessed frame 296/900...\nProcessed frame 300/900...\nProcessed frame 304/900...\nProcessed frame 308/900...\nProcessed frame 312/900...\nProcessed frame 316/900...\nProcessed frame 320/900...\nProcessed frame 324/900...\nProcessed frame 328/900...\nProcessed frame 332/900...\nProcessed frame 336/900...\nProcessed frame 340/900...\nProcessed frame 344/900...\nProcessed frame 348/900...\nProcessed frame 352/900...\nProcessed frame 356/900...\nProcessed frame 360/900...\nProcessed frame 364/900...\nProcessed frame 368/900...\nProcessed frame 372/900...\nProcessed frame 376/900...\nProcessed frame 380/900...\nProcessed frame 384/900...\nProcessed frame 388/900...\nProcessed frame 392/900...\nProcessed frame 396/900...\nProcessed frame 400/900...\nProcessed frame 404/900...\nProcessed frame 408/900...\nProcessed frame 412/900...\nProcessed frame 416/900...\nProcessed frame 420/900...\nProcessed frame 424/900...\nProcessed frame 428/900...\nProcessed frame 432/900...\nProcessed frame 436/900...\nProcessed frame 440/900...\nProcessed frame 444/900...\nProcessed frame 448/900...\nProcessed frame 452/900...\nProcessed frame 456/900...\nProcessed frame 460/900...\nProcessed frame 464/900...\nProcessed frame 468/900...\nProcessed frame 472/900...\nProcessed frame 476/900...\nProcessed frame 480/900...\nProcessed frame 484/900...\nProcessed frame 488/900...\nProcessed frame 492/900...\nProcessed frame 496/900...\nProcessed frame 500/900...\nProcessed frame 504/900...\nProcessed frame 508/900...\nProcessed frame 512/900...\nProcessed frame 516/900...\nProcessed frame 520/900...\nProcessed frame 524/900...\nProcessed frame 528/900...\nProcessed frame 532/900...\nProcessed frame 536/900...\nProcessed frame 540/900...\nProcessed frame 544/900...\nProcessed frame 548/900...\nProcessed frame 552/900...\nProcessed frame 556/900...\nProcessed frame 560/900...\nProcessed frame 564/900...\nProcessed frame 568/900...\nProcessed frame 572/900...\nProcessed frame 576/900...\nProcessed frame 580/900...\nProcessed frame 584/900...\nProcessed frame 588/900...\nProcessed frame 592/900...\nProcessed frame 596/900...\nProcessed frame 600/900...\nProcessed frame 604/900...\nProcessed frame 608/900...\nProcessed frame 612/900...\nProcessed frame 616/900...\nProcessed frame 620/900...\nProcessed frame 624/900...\nProcessed frame 628/900...\nProcessed frame 632/900...\nProcessed frame 636/900...\nProcessed frame 640/900...\nProcessed frame 644/900...\nProcessed frame 648/900...\nProcessed frame 652/900...\nProcessed frame 656/900...\nProcessed frame 660/900...\nProcessed frame 664/900...\nProcessed frame 668/900...\nProcessed frame 672/900...\nProcessed frame 676/900...\nProcessed frame 680/900...\nProcessed frame 684/900...\nProcessed frame 688/900...\nProcessed frame 692/900...\nProcessed frame 696/900...\nProcessed frame 700/900...\nProcessed frame 704/900...\nProcessed frame 708/900...\nProcessed frame 712/900...\nProcessed frame 716/900...\nProcessed frame 720/900...\nProcessed frame 724/900...\nProcessed frame 728/900...\nProcessed frame 732/900...\nProcessed frame 736/900...\nProcessed frame 740/900...\nProcessed frame 744/900...\nProcessed frame 748/900...\nProcessed frame 752/900...\nProcessed frame 756/900...\nProcessed frame 760/900...\nProcessed frame 764/900...\nProcessed frame 768/900...\nProcessed frame 772/900...\nProcessed frame 776/900...\nProcessed frame 780/900...\nProcessed frame 784/900...\nProcessed frame 788/900...\nProcessed frame 792/900...\nProcessed frame 796/900...\nProcessed frame 800/900...\nProcessed frame 804/900...\nProcessed frame 808/900...\nProcessed frame 812/900...\nProcessed frame 816/900...\nProcessed frame 820/900...\nProcessed frame 824/900...\nProcessed frame 828/900...\nProcessed frame 832/900...\nProcessed frame 836/900...\nProcessed frame 840/900...\nProcessed frame 844/900...\nProcessed frame 848/900...\nProcessed frame 852/900...\nProcessed frame 856/900...\nProcessed frame 860/900...\nProcessed frame 864/900...\nProcessed frame 868/900...\nProcessed frame 872/900...\nProcessed frame 876/900...\nProcessed frame 880/900...\nProcessed frame 884/900...\nProcessed frame 888/900...\nProcessed frame 892/900...\nProcessed frame 896/900...\nProcessed frame 900/900...\n\n--- Analysis Complete ---\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# --- IMPORTS ---\nimport torch\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon\nimport time\nfrom collections import defaultdict, deque\nimport pandas as pd\n\n# --- CONFIGURATION ---\nprint(\"\\n--- Initializing Configuration ---\")\n\n# --- CORE PARAMETERS ---\nMODEL_PATH = '/kaggle/working/runs/train/yolov8l_dropout_run/weights/best.pt'\nVIDEO_PATH = '/kaggle/input/videohai/traffic.mp4'\nOUTPUT_VIDEO_PATH = '/kaggle/working/traffic_analysis_final.mp4'\nCSV_LOG_PATH = '/kaggle/working/traffic_events_log_final.csv'\n\n# --- DETECTION & TRACKING ---\nTARGET_CLASSES = [0, 1, 2, 3] # 'car', 'truck', 'bus', 'motorcycle'\nCLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle']\nCONF_THRESHOLD = 0.3\nIOU_THRESHOLD = 0.5\nDEEPSORT_CONFIG = {'max_age': 50, 'n_init': 5, 'nms_max_overlap': 1.0, 'nn_budget': 100}\nBATCH_SIZE = 4\n\n# --- ANALYSIS & VISUALIZATION ---\n# Lane definitions\nLANE_LINE_DEFINITIONS = [\n    [(100, 720), (500, 450)],  # Lane 1 Left\n    [(550, 720), (650, 450)],  # Lane 1 Right / Lane 2 Left\n    [(800, 720), (850, 450)],  # Lane 2 Right / Lane 3 Left\n    [(1200, 720), (1100, 450)] # Lane 3 Right\n]\n# **IMPORTANT**: Define the real-world length of the lanes in METERS for speed estimation.\nREAL_WORLD_LANE_LENGTH_METERS = 20\nSPEED_UNIT = 'km/h' # or 'mph'\n\n# Thresholds for stopped vehicle and congestion analysis\nCONGESTION_THRESHOLD = 5  # Recommended: 6-10 for urban roads, 10-15 for highways\nSTOPPED_VEHICLE_SPEED_THRESHOLD_KMPH = 1.5  # 0–2 km/h is effectively stopped\nSTOPPED_VEHICLE_DURATION_SECONDS = 1.5  # Lower this for quicker response; 3–5 sec is good\n\n# --- REMOVED --- The LANE_FLOW_DIRECTION parameter has been removed as it was causing incorrect detections.\n\n# --- INITIALIZATION ---\nprint(\"--- Initializing Model, Tracker, and Video I/O ---\")\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = YOLO(MODEL_PATH)\nmodel.to(device)\ntracker = DeepSort(max_age=DEEPSORT_CONFIG['max_age'], n_init=DEEPSORT_CONFIG['n_init'], nms_max_overlap=DEEPSORT_CONFIG['nms_max_overlap'], nn_budget=DEEPSORT_CONFIG['nn_budget'])\ncap = cv2.VideoCapture(VIDEO_PATH)\nif not cap.isOpened(): raise IOError(f\"Error opening video: {VIDEO_PATH}\")\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nout = cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n\n# --- DATA STRUCTURES ---\nlane_polygons = [Polygon(LANE_LINE_DEFINITIONS[i] + LANE_LINE_DEFINITIONS[i+1][::-1]) for i in range(len(LANE_LINE_DEFINITIONS) - 1)]\ntrack_history = defaultdict(lambda: deque(maxlen=int(fps)))\nstopped_timers = defaultdict(int)\nevent_log = []\n\n# --- MAIN PROCESSING LOOP ---\nprint(f\"--- Starting Refined Video Analysis ({total_frames} frames) ---\")\nframes_batch = []\nframe_num = 0\nstart_time = time.time()\n\ntry:\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret: break\n        frames_batch.append(frame)\n        frame_num += 1\n\n        if len(frames_batch) == BATCH_SIZE or frame_num == total_frames:\n            results_batch = model.predict(frames_batch, classes=TARGET_CLASSES, conf=CONF_THRESHOLD, iou=IOU_THRESHOLD, verbose=False)\n\n            for i, results in enumerate(results_batch):\n                current_frame_index = frame_num - len(frames_batch) + i\n                current_frame_time = current_frame_index / fps\n                current_frame = frames_batch[i]\n                \n                detections = []\n                for box in results.boxes:\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    detections.append(([int(x1), int(y1), int(x2 - x1), int(y2 - y1)], box.conf[0].item(), CLASS_NAMES[int(box.cls[0].item())]))\n                \n                tracks = tracker.update_tracks(detections, frame=current_frame)\n                current_lane_counts = {lane: 0 for lane in range(1, len(lane_polygons) + 1)}\n                dashboard = np.zeros_like(current_frame, dtype=np.uint8)\n\n                for track in tracks:\n                    if not track.is_confirmed() or track.time_since_update > 1: continue\n                    \n                    track_id, bbox, class_name = track.track_id, track.to_ltrb(), track.get_det_class()\n                    bottom_center = Point((bbox[0] + bbox[2]) / 2, bbox[3])\n                    \n                    track_history[track_id].append(bottom_center)\n                    speed_kmph = -1\n                    if len(track_history[track_id]) > int(fps / 2):\n                        prev_pos = track_history[track_id][0]\n                        pixel_dist = bottom_center.distance(prev_pos)\n                        time_diff = len(track_history[track_id]) / fps\n                        perspective_scale = 1 - (bottom_center.y / h)\n                        real_world_dist = pixel_dist * perspective_scale * (REAL_WORLD_LANE_LENGTH_METERS / h)\n                        speed_mps = real_world_dist / time_diff if time_diff > 0 else 0\n                        speed_kmph = speed_mps * (3.6 if SPEED_UNIT == 'km/h' else 2.23694)\n                    \n                    assigned_lane = next((i + 1 for i, poly in enumerate(lane_polygons) if poly.contains(bottom_center)), None)\n                    if assigned_lane: current_lane_counts[assigned_lane] += 1\n\n                    # --- MODIFIED: Simplified Analysis and Visualization ---\n                    is_stopped = False\n                    \n                    # Stopped vehicle check\n                    if speed_kmph != -1 and speed_kmph < STOPPED_VEHICLE_SPEED_THRESHOLD_KMPH:\n                        stopped_timers[track_id] += 1\n                        if (stopped_timers[track_id] / fps) > STOPPED_VEHICLE_DURATION_SECONDS:\n                            is_stopped = True\n                            if not any(e['event'] == 'stopped_vehicle' and e['track_id'] == track_id for e in event_log):\n                                event_log.append({'timestamp': current_frame_time, 'event': 'stopped_vehicle', 'track_id': track_id, 'lane': assigned_lane, 'duration_s': STOPPED_VEHICLE_DURATION_SECONDS})\n                    else:\n                        stopped_timers[track_id] = 0\n                    \n                    # --- REMOVED --- Wrong-way check has been completely removed.\n\n                    # Set label and color based on stopped status\n                    if is_stopped:\n                        label = f\"{class_name} {track_id} [STOPPED]\"\n                        color = (0, 0, 255) # Red for stopped\n                    else:\n                        label = f\"{class_name} {track_id}\"\n                        if speed_kmph > 0:\n                            label += f\" {int(speed_kmph)} {SPEED_UNIT}\"\n                        color = (0, 255, 0) # Green for moving\n\n                    cv2.rectangle(current_frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n                    cv2.putText(current_frame, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n                \n                # Draw Dashboard and Lane Polygons\n                y_offset = 40\n                for lane_num, poly in enumerate(lane_polygons, 1):\n                    is_congested = current_lane_counts[lane_num] > CONGESTION_THRESHOLD\n                    lane_color = (0, 0, 255) if is_congested else (0, 255, 0)\n                    poly_pts = np.array(poly.exterior.coords, np.int32).reshape((-1, 1, 2))\n                    cv2.polylines(current_frame, [poly_pts], isClosed=True, color=lane_color, thickness=2)\n                    status = \"Congested\" if is_congested else \"Normal\"\n                    info_text = f\"Lane {lane_num} ({status}): {current_lane_counts[lane_num]} veh\"\n                    cv2.putText(dashboard, info_text, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n                    y_offset += 30\n                    if is_congested and not any(e['event'] == 'congestion' and e['lane'] == lane_num and abs(e['timestamp'] - current_frame_time) < 1 for e in event_log):\n                        event_log.append({'timestamp': current_frame_time, 'event': 'congestion', 'lane': lane_num, 'vehicle_count': current_lane_counts[lane_num]})\n                \n                final_frame = cv2.addWeighted(current_frame, 1, dashboard, 0.8, 0)\n                out.write(final_frame)\n\n            frames_batch = []\n            elapsed_time = time.time() - start_time\n            processed_frames = frame_num - len(frames_batch)\n            fps_current = processed_frames / elapsed_time if elapsed_time > 0 else 0\n            print(f\"Processed frame {processed_frames}/{total_frames}... Current FPS: {fps_current:.2f}\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    cap.release()\n    out.release()\n    print(f\"\\n--- Analysis Complete ---\")\n    print(f\"Output video saved to: {OUTPUT_VIDEO_PATH}\")\n    if event_log:\n        df = pd.DataFrame(event_log)\n        df.to_csv(CSV_LOG_PATH, index=False)\n        print(f\"Event log saved to: {CSV_LOG_PATH}\")\n    else:\n        print(\"No critical events were logged.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.024306Z","iopub.status.idle":"2025-06-17T16:44:59.024632Z","shell.execute_reply.started":"2025-06-17T16:44:59.024458Z","shell.execute_reply":"2025-06-17T16:44:59.024472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CODE CELL 2: THE DEFINITIVE ANALYSIS SCRIPT\n\n# --- 1. IMPORTS AND THE GUARANTEED PATH FIX ---\nimport sys\nimport os\n\n# --- THE GUARANTEED FIX FOR THE IMPORT ERROR ---\n# We temporarily change the working directory to the UFLD folder\n# to make the import unambiguous. This is the most robust method.\noriginal_cwd = os.getcwd()  # Save our current directory\nufld_path = '/kaggle/working/Ultra-Fast-Lane-Detection'\ntry:\n    os.chdir(ufld_path)\n    # This import will now definitely find the correct model.py\n    from model import parsingNet\n    print(\"✅ UFLD model definition imported successfully.\")\nfinally:\n    # IMPORTANT: Change back to the original directory so all other paths work correctly\n    os.chdir(original_cwd)\n# --- END OF FIX ---\n\nimport cv2\nimport torch\nimport numpy as np\nimport json\nimport time\nfrom collections import deque, defaultdict\nfrom shapely.geometry import Point, Polygon\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nprint(\"✅ All other libraries imported successfully.\")\n\n\n# --- 2. CONFIGURATION ---\nprint(\"\\n--- Loading Configuration ---\")\nYOLO_MODEL_PATH = 'yolov8l.pt'\nLANE_MODEL_PATH = '/kaggle/input/ufld-tusimple-model/tusimple_18.pth'\nINPUT_VIDEO_PATH = '/kaggle/input/test-video-data/test_vid.mp4' # <-- UPDATE THIS IF NEEDED\n\nOUTPUT_DIR = '/kaggle/working/output_analysis'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\noutput_video_path = os.path.join(OUTPUT_DIR, f\"analyzed_{os.path.basename(INPUT_VIDEO_PATH)}\")\noutput_json_path = os.path.join(OUTPUT_DIR, f\"analysis_log_{os.path.splitext(os.path.basename(INPUT_VIDEO_PATH))[0]}.json\")\nCONF_THRESHOLD = 0.40\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\n# --- 3. UTILITY FUNCTIONS ---\ndef initialize_models():\n    print(\"Initializing models...\")\n    vehicle_model = YOLO(YOLO_MODEL_PATH); vehicle_model.to(device)\n    lane_model = parsingNet(pretrained=False, backbone='18', cls_dim=(201, 18, 4), use_aux=False).to(device)\n    if not os.path.exists(LANE_MODEL_PATH):\n        raise FileNotFoundError(f\"UFLD model weights not found at {LANE_MODEL_PATH}. Did you add the Kaggle dataset?\")\n    state_dict = torch.load(LANE_MODEL_PATH, map_location='cpu')['model']\n    lane_model.load_state_dict(state_dict); lane_model.eval()\n    tracker = DeepSort(max_age=30, nn_budget=70, n_init=3)\n    print(\"✅ All models initialized successfully.\")\n    return vehicle_model, lane_model, tracker\n\ndef process_frame_for_lanes(frame, lane_model):\n    img_h, img_w, _ = frame.shape\n    img_for_lane = cv2.resize(frame, (288, 800)); img_for_lane = cv2.cvtColor(img_for_lane, cv2.COLOR_BGR2RGB)\n    img_for_lane = torch.from_numpy(img_for_lane.transpose((2, 0, 1))).float().unsqueeze(0).to(device) / 255.0\n    with torch.no_grad(): out = lane_model(img_for_lane)\n    col_sample = np.linspace(0, 288 - 1, 200); col_sample_w = col_sample[1] - col_sample[0]\n    out_j = out[0].data.cpu().numpy()[:, ::-1, :]; prob = torch.softmax(torch.from_numpy(out_j), dim=0)\n    idx = torch.arange(201, dtype=torch.float32).unsqueeze(1).unsqueeze(2); loc = torch.sum(prob * idx, dim=0)\n    out_j = loc.numpy() * col_sample_w * img_w / 288\n    lanes = []; row_anchor = np.linspace(590, 710, 18)\n    for i in range(out_j.shape[1]):\n        lane_points = out_j[:, i]; valid_points_mask = lane_points > 0\n        if np.sum(valid_points_mask) > 2:\n            lane = [];\n            for k, is_valid in enumerate(valid_points_mask):\n                if is_valid: lane.append((int(lane_points[k]), int(row_anchor[k] * img_h / 720.0)))\n            if lane: lanes.append(lane)\n    return lanes\n\ndef create_lane_polygons(lanes):\n    if len(lanes) < 2: return []\n    lanes = sorted([l for l in lanes if l], key=lambda l: l[0][0])\n    if len(lanes) < 2: return []\n    return [Polygon(lanes[i] + lanes[i+1][::-1]) for i in range(len(lanes) - 1)]\n\ndef get_vehicle_lane_assignment(bbox, lane_polygons):\n    bottom_center = Point((bbox[0] + bbox[2]) / 2, bbox[3])\n    for i, poly in enumerate(lane_polygons):\n        if poly.contains(bottom_center): return i + 1\n    return None\n\ndef draw_text_with_outline(img, text, origin, font=cv2.FONT_HERSHEY_SIMPLEX, scale=0.7, color=(255, 255, 255), thickness=2):\n    (text_w, text_h), _ = cv2.getTextSize(text, font, scale, thickness)\n    box_coords = ((origin[0] - 5, origin[1] + 5), (origin[0] + text_w + 5, origin[1] - text_h - 5))\n    overlay = img.copy(); cv2.rectangle(overlay, box_coords[0], box_coords[1], (0, 0, 0), -1)\n    img = cv2.addWeighted(overlay, 0.6, img, 0.4, 0)\n    cv2.putText(img, text, origin, font, scale, color, thickness, cv2.LINE_AA)\n    return img\n\n# --- 4. MAIN PROCESSING ---\nvehicle_model, lane_model, tracker = initialize_models()\ncap = cv2.VideoCapture(INPUT_VIDEO_PATH);\nif not cap.isOpened(): print(f\"Error: Could not open video {INPUT_VIDEO_PATH}\"); sys.exit(1)\nw=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); h=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)); fps=int(cap.get(cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\nframe_number = 0; full_analysis_log = {}; track_history = defaultdict(lambda: deque(maxlen=30)); lane_vehicle_ids = defaultdict(set);\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nprint(f\"\\n--- Starting video analysis pipeline on {total_frames} frames ---\"); start_time = time.time()\n\nwhile cap.isOpened():\n    ret, frame = cap.read();\n    if not ret: break\n    frame_number += 1\n    if frame_number % 50 == 0: print(f\"Processing frame {frame_number}/{total_frames}...\")\n\n    detected_lanes = process_frame_for_lanes(frame, lane_model); lane_polygons = create_lane_polygons(detected_lanes)\n    vehicle_results = vehicle_model(frame, conf=CONF_THRESHOLD, classes=[2, 3, 5, 7])\n    detections_for_tracker = [[(det.xyxy[0].cpu().numpy()), det.conf[0].item(), int(det.cls[0].item())] for det in vehicle_results[0].boxes]\n    detections_for_tracker = [([d[0][0], d[0][1], d[0][2]-d[0][0], d[0][3]-d[0][1]], d[1], d[2]) for d in detections_for_tracker]\n    tracks = tracker.update_tracks(detections_for_tracker, frame=frame)\n\n    current_frame_analysis = {'frame_id': frame_number, 'vehicles': [], 'lane_analysis': {}}; current_lane_counts = defaultdict(int)\n    annotated_frame = frame.copy()\n    \n    for poly in lane_polygons:\n        pts = np.array(poly.exterior.coords, np.int32).reshape((-1, 1, 2))\n        overlay = annotated_frame.copy(); cv2.fillPoly(overlay, [pts], (0, 255, 255));\n        annotated_frame = cv2.addWeighted(overlay, 0.15, annotated_frame, 0.85, 0)\n        cv2.polylines(annotated_frame, [pts], isClosed=True, color=(255, 255, 0), thickness=2)\n\n    for track in tracks:\n        if not track.is_confirmed() or track.time_since_update > 2: continue\n        track_id = track.track_id; ltrb = track.to_ltrb(); bbox = [int(v) for v in ltrb]\n        class_id = track.get_det_class(); class_name = vehicle_model.names[class_id]\n        center_point = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2); track_history[track_id].append(center_point)\n        speed_px_per_frame = 0;\n        if len(track_history[track_id]) > 5:\n            prev_point = track_history[track_id][0]; dx = center_point[0] - prev_point[0]; dy = center_point[1] - prev_point[1]\n            speed_px_per_frame = np.sqrt(dx**2 + dy**2) / len(track_history[track_id])\n        is_stopped = speed_px_per_frame < 1.0 and track.age > fps\n        assigned_lane = get_vehicle_lane_assignment(bbox, lane_polygons)\n        if assigned_lane: current_lane_counts[assigned_lane] += 1; lane_vehicle_ids[assigned_lane].add(track_id)\n        \n        color = (0, 0, 255) if is_stopped else (0, 255, 0); label = f\"ID:{track_id} {class_name}\"\n        if assigned_lane: label += f\" L:{assigned_lane}\"\n        cv2.rectangle(annotated_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n        annotated_frame = draw_text_with_outline(annotated_frame, label, (bbox[0], bbox[1] - 10), color=color)\n\n    for i in range(len(lane_polygons)):\n        lane_id = i + 1; num_vehicles = current_lane_counts.get(lane_id, 0)\n        congestion = \"High\" if num_vehicles > 10 else \"Medium\" if num_vehicles > 5 else \"Low\"\n        current_frame_analysis['lane_analysis'][f'lane_{lane_id}'] = {'vehicle_count':num_vehicles, 'total_vehicles_passed':len(lane_vehicle_ids[lane_id]), 'congestion_level':congestion}\n    \n    y_offset = 30\n    annotated_frame = draw_text_with_outline(annotated_frame, f\"Frame: {frame_number}\", (10, y_offset)); y_offset += 35\n    annotated_frame = draw_text_with_outline(annotated_frame, f\"Total Vehicles: {len(tracks)}\", (10, y_offset));\n    for i in range(len(lane_polygons)):\n        lane_id=i+1; lane_info=current_frame_analysis['lane_analysis'].get(f'lane_{lane_id}',{}); y_offset += 35\n        text=f\"Lane {lane_id}: {lane_info.get('vehicle_count',0)} ({lane_info.get('congestion_level','N/A')}) | Total: {lane_info.get('total_vehicles_passed',0)}\";\n        annotated_frame = draw_text_with_outline(annotated_frame, text, (10, y_offset))\n        \n    video_writer.write(annotated_frame);\n    full_analysis_log[f'frame_{frame_number}'] = current_frame_analysis\n\ncap.release(); video_writer.release(); end_time = time.time()\nwith open(output_json_path, 'w') as f: json.dump(full_analysis_log, f, indent=4)\nprint(\"\\n--- ✅ Analysis Complete ---\"); print(f\"Processing time: {end_time - start_time:.2f} seconds\")\nprint(f\"Annotated video saved to: {os.path.abspath(output_video_path)}\")\nprint(f\"Detailed JSON log saved to: {os.path.abspath(output_json_path)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.026048Z","iopub.status.idle":"2025-06-17T16:44:59.026250Z","shell.execute_reply.started":"2025-06-17T16:44:59.026157Z","shell.execute_reply":"2025-06-17T16:44:59.026165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CODE CELL 2: SELF-CONTAINED TRAINING\n# --- 1. INSTALL DEPENDENCIES FOR THIS CELL ---\n!pip install ultralytics -q\n\n# --- 2. IMPORT AND TRAIN ---\nfrom ultralytics import YOLO\n\n# Define paths\nCUSTOM_MODEL_YAML_PATH = '/kaggle/working/yolov8l_dropout.yaml'\nDATA_YAML_PATH = '/kaggle/working/vehicle_data.yaml'\n\nprint(\"Dependencies installed and import successful.\")\nprint(\"Loading model from configuration...\")\nmodel = YOLO(CUSTOM_MODEL_YAML_PATH)\n\nprint(\"Starting training with Transfer Learning...\")\nresults = model.train(\n    data=DATA_YAML_PATH,\n    pretrained='yolov8l.pt',\n    imgsz=640,\n    batch=8, # A smaller batch size is needed for larger models\n    epochs=100,\n    name='yolov8l_dropout_run',\n    project='/kaggle/working/runs/train',\n    cache=True,\n    patience=30,\n)\n\nprint(f\"\\n--- Training Complete ---\")\n# The path to the best model will be printed at the end of training.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.027519Z","iopub.status.idle":"2025-06-17T16:44:59.027736Z","shell.execute_reply.started":"2025-06-17T16:44:59.027640Z","shell.execute_reply":"2025-06-17T16:44:59.027649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CODE CELL 4: TRAINING\nimport sys\nimport os\n\n# --- THE GUARANTEED FIX ---\nULTRALYTICS_PATH = '/kaggle/working/ultralytics'\nif ULTRALYTICS_PATH not in sys.path:\n    sys.path.insert(0, ULTRALYTICS_PATH)\nprint(f\"Ensured '{ULTRALYTICS_PATH}' is at the front of sys.path.\")\n\nfrom ultralytics import YOLO\n\n# Define the paths\nCUSTOM_MODEL_YAML_PATH = '/kaggle/working/yolov8s_max_custom.yaml'\nDATA_YAML_PATH = '/kaggle/working/vehicle_data.yaml'\n\nprint(\"Import successful. Loading model...\")\nmodel = YOLO(CUSTOM_MODEL_YAML_PATH)\n\nprint(\"Starting training...\")\nresults = model.train(\n    data=DATA_YAML_PATH,\n    imgsz=640,\n    batch=16,\n    epochs=150,\n    name='yolov8_multi_class_run',\n    project='/kaggle/working/runs/train',\n    cache=True,\n    patience=50,\n)\n\nBEST_MODEL_PATH = '/kaggle/working/runs/train/yolov8_multi_class_run/weights/best.pt'\nprint(f\"\\n--- Training Complete ---\")\nprint(f\"Best model weights saved at: {BEST_MODEL_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.028550Z","iopub.status.idle":"2025-06-17T16:44:59.028857Z","shell.execute_reply.started":"2025-06-17T16:44:59.028693Z","shell.execute_reply":"2025-06-17T16:44:59.028707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CODE CELL 4: ANALYSIS\nimport sys\nimport os\nimport cv2\nimport torch\nimport numpy as np\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon\n\n# Add the sys.path fix here too, just in case you run this cell independently\nULTRALYTICS_PATH = '/kaggle/working/ultralytics'\nif ULTRALYTICS_PATH not in sys.path:\n    sys.path.insert(0, ULTRALYTICS_PATH)\n\nfrom ultralytics import YOLO\n\n# --- CONFIGURATION ---\nBEST_MODEL_PATH = '/kaggle/working/runs/train/yolov8_multi_class_run/weights/best.pt'\nFINAL_CLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle']\nVIDEO_PATH = '/kaggle/input/traffic-dataset/test_vid.mp4'\nLANE_LINES = [[(100,720),(500,450)],[(550,720),(650,450)],[(800,720),(850,450)],[(1200,720),(1100,450)]]\nOUTPUT_VIDEO_PATH = '/kaggle/working/final_multiclass_analysis.mp4'\nCONF_THRESHOLD = 0.35\n\n# --- INITIALIZE & PROCESS ---\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\nmodel=YOLO(BEST_MODEL_PATH); model.to(device)\ntracker=DeepSort(max_age=30,nn_budget=50,n_init=3)\ncap=cv2.VideoCapture(VIDEO_PATH)\nif not cap.isOpened(): print(f\"Error opening video {VIDEO_PATH}\")\nelse:\n    w,h,fps=(int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH,cv2.CAP_PROP_FRAME_HEIGHT,cv2.CAP_PROP_FPS))\n    out=cv2.VideoWriter(OUTPUT_VIDEO_PATH,cv2.VideoWriter_fourcc(*'mp4v'),fps,(w,h))\n    lane_polygons=[Polygon(LANE_LINES[i]+LANE_LINES[i+1][::-1]) for i in range(len(LANE_LINES)-1)]\n    lane_vehicle_ids={i+1:{cls:set() for cls in FINAL_CLASS_NAMES} for i in range(len(lane_polygons))}\n    frame_num=0\n    while cap.isOpened():\n        ret,frame=cap.read();\n        if not ret:break\n        if frame_num%100==0:print(f\"Processing frame {frame_num}/{int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}...\")\n        frame_num+=1\n        results=model(frame,verbose=False)\n        detections=[]\n        for box in results[0].boxes:\n            if box.conf[0]>CONF_THRESHOLD:x1,y1,x2,y2=box.xyxy[0].cpu().numpy();detections.append(([int(x1),int(y1),int(x2-x1),int(y2-y1)],box.conf[0],int(box.cls[0])))\n        tracks=tracker.update_tracks(detections,frame=frame)\n        for track in tracks:\n            if not track.is_confirmed() or track.time_since_update>2:continue\n            track_id,bbox,cls_id=track.track_id,track.to_ltrb(orig_image_wh=(w,h)),track.get_det_class();class_name=FINAL_CLASS_NAMES[cls_id]\n            assigned_lane=next((i+1 for i,poly in enumerate(lane_polygons) if poly.contains(Point((bbox[0]+bbox[2])/2,bbox[3]))),None)\n            if assigned_lane:lane_vehicle_ids[assigned_lane][class_name].add(track_id)\n            label=f\"{class_name} {track_id}\"+(f\" L:{assigned_lane}\" if assigned_lane else \"\");cv2.rectangle(frame,(int(bbox[0]),int(bbox[1])),(int(bbox[2]),int(bbox[3])),(0,255,0),2);cv2.putText(frame,label,(int(bbox[0]),int(bbox[1])-10),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,255,0),2)\n        y_offset=40\n        for lane_num,class_counts in lane_vehicle_ids.items():\n            total_in_lane=sum(len(ids) for ids in class_counts.values());cv2.putText(frame,f\"Lane {lane_num} Total: {total_in_lane}\",(20,y_offset),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),3);y_offset+=30\n            breakdown=\", \".join([f\"{name}:{len(ids)}\" for name,ids in class_counts.items() if len(ids)>0]);cv2.putText(frame,breakdown,(30,y_offset),cv2.FONT_HERSHEY_SIMPLEX,0.6,(200,200,200),2);y_offset+=40\n        out.write(frame)\n    cap.release();out.release();print(f\"\\n--- Analysis Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.029938Z","iopub.status.idle":"2025-06-17T16:44:59.030215Z","shell.execute_reply.started":"2025-06-17T16:44:59.030067Z","shell.execute_reply":"2025-06-17T16:44:59.030080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CODE CELL 5\nimport cv2\nimport torch\nimport numpy as np\nfrom ultralytics import YOLO\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\nfrom shapely.geometry import Point, Polygon\n\n# Define the class names again after restart\nFINAL_CLASS_NAMES = ['car', 'truck', 'bus', 'motorcycle']\n\n# Read the best model path from the file\nwith open(\"/kaggle/working/best_model_path.txt\", \"r\") as f:\n    YOLO_MODEL_PATH = f.read().strip()\n\nprint(f\"Using model from: {YOLO_MODEL_PATH}\")\n\n# --- ACTION REQUIRED: CONFIGURE THESE ---\nVIDEO_PATH = '/kaggle/input/traffic-dataset/test_vid.mp4'\nLANE_LINES = [[(100,720),(500,450)],[(550,720),(650,450)],[(800,720),(850,450)],[(1200,720),(1100,450)]]\n# ----------------------------------------\n\nCLASS_NAMES_FROM_TRAINING = FINAL_CLASS_NAMES\nOUTPUT_VIDEO_PATH = '/kaggle/working/final_multiclass_analysis.mp4'\nCONF_THRESHOLD = 0.35\n\n# --- INITIALIZE MODELS ---\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\nmodel=YOLO(YOLO_MODEL_PATH); model.to(device)\ntracker=DeepSort(max_age=30,nn_budget=50,n_init=3)\n\n# --- MAIN PROCESSING LOOP ---\ncap = cv2.VideoCapture(VIDEO_PATH)\nif not cap.isOpened(): print(f\"Error: Could not open video {VIDEO_PATH}\")\nelse:\n    w,h,fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n    out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n    lane_polygons = [Polygon(lane_lines[i]+lane_lines[i+1][::-1]) for i in range(len(lane_lines)-1)]\n    lane_vehicle_ids = {i+1: {cls: set() for cls in CLASS_NAMES_FROM_TRAINING} for i in range(len(lane_polygons))}\n\n    frame_num = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret: break\n        if frame_num % 100 == 0: print(f\"Processing frame {frame_num}/{int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}...\")\n        frame_num += 1\n\n        results = model(frame, verbose=False)\n        detections = []\n        for box in results[0].boxes:\n            if box.conf[0] > CONF_THRESHOLD:\n                x1,y1,x2,y2=box.xyxy[0].cpu().numpy()\n                detections.append(([int(x1),int(y1),int(x2-x1),int(y2-y1)],box.conf[0],int(box.cls[0])))\n        \n        tracks = tracker.update_tracks(detections, frame=frame)\n        \n        for track in tracks:\n            if not track.is_confirmed() or track.time_since_update > 2: continue\n            track_id, bbox, cls_id = track.track_id, track.to_ltrb(orig_image_wh=(w,h)), track.get_det_class()\n            class_name = CLASS_NAMES_FROM_TRAINING[cls_id]\n            assigned_lane = get_vehicle_lane(bbox, lane_polygons)\n            if assigned_lane: lane_vehicle_ids[assigned_lane][class_name].add(track_id)\n            label = f\"{class_name} {track_id}\" + (f\" L:{assigned_lane}\" if assigned_lane else \"\")\n            cv2.rectangle(frame,(int(bbox[0]),int(bbox[1])),(int(bbox[2]),int(bbox[3])),(0,255,0),2)\n            cv2.putText(frame,label,(int(bbox[0]),int(bbox[1])-10),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,255,0),2)\n        \n        y_offset = 40\n        for lane_num, class_counts in lane_vehicle_ids.items():\n            total_in_lane = sum(len(ids) for ids in class_counts.values())\n            cv2.putText(frame,f\"Lane {lane_num} Total: {total_in_lane}\",(20,y_offset),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),3)\n            y_offset += 30\n            breakdown = \", \".join([f\"{name}: {len(ids)}\" for name,ids in class_counts.items() if len(ids)>0])\n            cv2.putText(frame,breakdown,(30,y_offset),cv2.FONT_HERSHEY_SIMPLEX,0.6,(200,200,200),2)\n            y_offset += 40\n        out.write(frame)\n\n    cap.release(); out.release()\n    print(f\"\\n--- Analysis Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:44:59.031090Z","iopub.status.idle":"2025-06-17T16:44:59.031313Z","shell.execute_reply.started":"2025-06-17T16:44:59.031202Z","shell.execute_reply":"2025-06-17T16:44:59.031216Z"}},"outputs":[],"execution_count":null}]}